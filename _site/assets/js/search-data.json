{"0": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Reward Documentation (2020)",
    "content": "Last updated: July 1, 2020 Written by Anna Xu . Reward is a dataset comprised of 6 projects collected over the span of years, totaling 509 participants (note that some participants were present in multiple projects). These are the 6 projects with the scans available in them: . | FNDM1: T1w, ASL, b0 phase 1 and 2, b0 magnitude, resting-state, task fMRI (face run 1 and 2; card run 1 and 2) | FNDM2: T1w, b0 phase 1 and 2, b0 magnitude, resting-state, task fMRI (itc runs 1-4) | NEFF: T1w, b0 phasediff, b0 magnitude, task fMRI (effort runs 1-4) . | Note that 15 participants from FNDM2 have been reclassified into NEFF because they are neff pilots identified by Dan | . | NEFF2: T1w, b0 phasediff, b0 magnitude, task fMRI (effort runs 1-4) | NODRA: T1w, b0 phasediff, b0 magnitude, resting-state, task fMRI (card runs 1-2, itc runs 1-2) . | Note that some participants have multiband resting-state scans while others have single band or both | . | DAY2: T1w, ASL, b0 phase 1 and 2, b0 magnitude, resting-state, task fMRI (face run 1 and 2; card run 1 and 2), and DTI for very few participants (n&lt;20) | . ",
    "url": "http://localhost:4000/anna/#reward-documentation-2020",
    "relUrl": "/anna/#reward-documentation-2020"
  },"1": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Imaging &amp; Task Data Status (Flywheel)",
    "content": " ",
    "url": "http://localhost:4000/anna/#imaging--task-data-status-flywheel",
    "relUrl": "/anna/#imaging--task-data-status-flywheel"
  },"2": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Scan data &amp; associated task behavioral data",
    "content": "Completeness . All scan data found on XNAT and associated task behavioral data found on XNAT or CfN has been uploaded onto Flywheel in the project Reward2018. Scan data was de-identified by uploading with the command fw import dicom ${dicomUploadPath} mcieslak \"Reward2018\" --subject ${bblid} --session ${subproject} -y --profile \"dicom_config.yaml\" in the command line and with the configuration file dicom_config.yaml found on Flywheel in Reward2018 &gt; Information. The following procedure checked to make sure all expected scans on XNAT were accounted for on Flywheel: . | All participants on XNAT were cross-checked with all participants on Flywheel to ensure that each expected participant in Reward had their scans on Flywheel. | For each individual participant, completeness of scans were checked. If a participant did not have all the scans expected based on the project they were from, the missing scans were checked for on XNAT. If they existed on XNAT, the scans were uploaded onto Flywheel. If they were not on XNAT, they were documented on an excel file found on Flywheel in Reward2018 &gt; information &gt; missingScans_20200624.xls with any notes from XNAT documented. | This excel file contains the sheet missingScanNotes which details the bblid, project, scan missing, reason for scan missing (either not on xnat, upload error, or excluded participant), notes from xnat, and notes I’ve written. | The other sheet on this spreadsheet is missingScanCount which contains the number of scans missing for each particular scan in each project. | . | . Scanid for each participant has also been noted in the custom info for each subject/session. Fw-heudiconv . These scans have also been fw’heudiconv-ed using these heuristics and the command fw-heudiconv-curate --project \"Reward2018\" --heuristic \"${heuristicFile}\" --session \"${subproject}\" --subject \"${bblid}\". You can also use the script subsetFwheudiconv.sh to mass run fw-heudiconv for a select few participants. fMRIPrep . Some scans have been processed with an old version of fMRIPrep but all scans may need to be fMRIPrep’d again with the latest version of fMRIPrep. runfMRIPrep.py details the previous process of running fMRIPrep. Task files . Files documenting all nifti scans and their associated task behavioral data can be found on Flywheel in the file fwScansTask_20200624.csv. This file is located in Reward2018 &gt; Information and contains the following variables: . | bblid, projects | scan_file: name of the nifti | bids: BIDS name of the nifti | file_type: type of scan (e.g., b0 magnitude, resting-state, etc.) | assoc_task_file: name of task file uploaded onto Flywheel that is associated with the particular scan | is_task: tells you whether the scan is a task fMRI scan | is_task_missing: tells you whether the associated task file for a task fMRI scan is missing. | . Another file, missingTaskFiles_20200625.csv documents a summary of the missing task files. This file is currently on the reward_data_mgmt slack channel (unsure if this is also on the Reward2018 project on Flywheel due to inability to check Flywheel before my departure). This file contains the following: . | is_missing: tells you whether the participant is missing is missing a task file, but since this spreadsheet is just of participants missing a task behavioral file, they all are | n(): tells you how many task behavioral files associated with a participant is missing | . If needed, scanAuditFlywheel.py goes through the process of generating a large spreadsheet of all files present in the Reward2018 project. To generate these associated files, use the script scanTaskDocumentation.R which sources code from classify_scans.R and classify_task.R. ",
    "url": "http://localhost:4000/anna/#scan-data--associated-task-behavioral-data",
    "relUrl": "/anna/#scan-data--associated-task-behavioral-data"
  },"3": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Other data",
    "content": "Demographics, Medication Status, Diagnosis . Demographics, medication status, diagnosis, and effort data are currently with Dan (not uploaded anywhere). Missing data has been previously reported in the reward_data_mgmt slack channel. Aylin’s Monster . Aylin’s monster scripts can be found in the Reward GitHub repo by clicking here. A wiki page of old documentation for Aylin’s monster can be found here. ",
    "url": "http://localhost:4000/anna/#other-data",
    "relUrl": "/anna/#other-data"
  },"4": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Legacy Curation (2018-2020)",
    "content": " ",
    "url": "http://localhost:4000/anna/",
    "relUrl": "/anna/"
  },"5": {
    "doc": "CogTrain",
    "title": "CogTrain",
    "content": "This section outlines progress with the CogTrain project. ",
    "url": "http://localhost:4000/projects/cogtrain/cogtrain_index/",
    "relUrl": "/projects/cogtrain/cogtrain_index/"
  },"6": {
    "doc": "Data Narrative",
    "title": "FNDM1",
    "content": ". | Data Processing Flow &amp; Important Links: | Plan for the Data | Data Acquisition | Download and Storage | Curation Process . | BIDS Validation: | BIDS Optimization: | . | Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Exemplar Testing: | Production Testing: | . | XCP-ABCD (version 0.0.8) . | Production Testing: | . | . | Post Processing . | To Do | . | . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#fndm1",
    "relUrl": "/projects/fndm1/datanarrative/#fndm1"
  },"7": {
    "doc": "Data Narrative",
    "title": "Data Processing Flow &amp; Important Links:",
    "content": ". | Flow diagram that describes the lifecycle of this dataset curation and preprocessing: | . | DSR GitHub Project Page(Curation/Validation and Processing Queue Status): | . https://github.com/PennLINC/Reward/projects/1 . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#data-processing-flow--important-links",
    "relUrl": "/projects/fndm1/datanarrative/#data-processing-flow--important-links"
  },"8": {
    "doc": "Data Narrative",
    "title": "Plan for the Data",
    "content": ". | Why does PennLINC need this data? Acquired at UPenn . | For which project(s) is it intended? Please link to project pages below: Reward Project . | Goal: Curate and preprocess an amalgam of datasets for a harmonized PennLINC resource . | . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#plan-for-the-data",
    "relUrl": "/projects/fndm1/datanarrative/#plan-for-the-data"
  },"9": {
    "doc": "Data Narrative",
    "title": "Data Acquisition",
    "content": ". | Data acquired by Dan Wolf &amp; Ted Satterthwaite | Describe the data: . | number of subjects = 56 | types of images = bold (2 runs task-CARD, 2 runs task-FACE, rest), T1w, fieldmaps | . | . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#data-acquisition",
    "relUrl": "/projects/fndm1/datanarrative/#data-acquisition"
  },"10": {
    "doc": "Data Narrative",
    "title": "Download and Storage",
    "content": ". | Original data available on Flywheel | Source data (NIfTI) on CUBIC in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/fndm1. | Data was copied to /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm1 and checked in to datalad after removing PHI (below) | JSON’s within origial_data were updated using cubids-add-nifti-info. | Listing metadata fields using cubids-print-metadata-fields gave the following fields: | . Acknowledgements AcquisitionDateTime AcquisitionMatrixPE AcquisitionNumber AcquisitionTime Authors BIDSVersion BandwidthPerPixelPhaseEncode BaseResolution CoilString ConversionSoftware ConversionSoftwareVersion DatasetDOI DeidentificationMethod DerivedVendorReportedEchoSpacing DeviceSerialNumber DwellTime EchoNumber EchoTime EchoTime1 EchoTime2 EchoTrainLength EffectiveEchoSpacing FlipAngle Funding HowToAcknowledge ImageOrientationPatientDICOM ImageType ImagingFrequency InPlanePhaseEncodingDirectionDICOM InstitutionAddress InstitutionName InstitutionalDepartmentName IntendedFor InversionTime License MRAcquisitionType MagneticFieldStrength Manufacturer ManufacturersModelName Modality Name ParallelReductionFactorInPlane PartialFourier PatientPosition PatientSex PercentPhaseFOV PhaseEncodingDirection PhaseEncodingSteps PhaseResolution PixelBandwidth ProcedureStepDescription ProtocolName PulseSequenceDetails ReceiveCoilName ReconMatrixPE RefLinesPE ReferencesAndLinks RepetitionTime SAR ScanOptions ScanningSequence SequenceName SequenceVariant SeriesDescription SeriesInstanceUID SeriesNumber ShimSetting SliceThickness SliceTiming SoftwareVersions SpacingBetweenSlices StationName StudyID StudyInstanceUID TaskName TotalReadoutTime TxRefAmp template ImageComments MultibandAccelerationFactor . Offending fields were removed with cubids-remove-metadata-fields in /cbica/projects/wolf_satterthwaite_reward/Curation/code/metadatafields_remove.sh . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#download-and-storage",
    "relUrl": "/projects/fndm1/datanarrative/#download-and-storage"
  },"11": {
    "doc": "Data Narrative",
    "title": "Curation Process",
    "content": ". | Data curation by Tinashe Tapera on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iteration7/fndm1/ | . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#curation-process",
    "relUrl": "/projects/fndm1/datanarrative/#curation-process"
  },"12": {
    "doc": "Data Narrative",
    "title": "BIDS Validation:",
    "content": ". | Data with short bold time series (&lt;3mins) were removed with the Notebook /cbica/projects/wolf_satterthwaite_reward/Curation/code/RemoveShortBOLD.ipynb . | All validation outputs are available in chronological order in /cbica/projects/wolf_satterthwaite_reward/Curation/code/validate_outputs/fndm1; most recent validation errors being: . | EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 224 counts . | README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 count . | NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 count . | . | . Data at this stage were approved for preprocessing. ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#bids-validation",
    "relUrl": "/projects/fndm1/datanarrative/#bids-validation"
  },"13": {
    "doc": "Data Narrative",
    "title": "BIDS Optimization:",
    "content": ". | All cubids optimization results are available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter&lt;ITERATION_NUMBER&gt;/fndm1/ . | Final optimization resulted in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter7/fndm1/fndm1_summary.csv . | . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#bids-optimization",
    "relUrl": "/projects/fndm1/datanarrative/#bids-optimization"
  },"14": {
    "doc": "Data Narrative",
    "title": "Preprocessing Pipelines",
    "content": " ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#preprocessing-pipelines",
    "relUrl": "/projects/fndm1/datanarrative/#preprocessing-pipelines"
  },"15": {
    "doc": "Data Narrative",
    "title": "fMRIPrep (version 20.2.3)",
    "content": ". | Tinashe Tapera was responsible for running preprocessing pipelines/audits on CUBIC | . Exemplar Testing: . | Used cubids to create exemplar dataset: cubids-copy-exemplars /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm1/BIDS /cbica/projects/wolf_satterthwaite_reward/Testing/fndm1/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter7/fndm1/fndm1_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Testing/fndm1/exemplars_dir | Path to fmriprep container: Original in ~/dropbox, datalad in /cbica/projects/wolf_satterthwaite_reward/Testing/exemplars_test/fmriprep-container . | Adjustments: . | During testing, some fieldmaps were found to be corrupt/unusable for the data, and were removed. These files are in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/fndm1/purge_broken_fmaps.txt | . | . Testing directory was deleted to save space on CUBIC on 12/2/21, once production completed . Production Testing: . | 52/56 subjects completed fMRIPrep successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm1/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/fmriprep/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/fmriprep/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/fmriprep/-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: NA | . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#fmriprep-version-2023",
    "relUrl": "/projects/fndm1/datanarrative/#fmriprep-version-2023"
  },"16": {
    "doc": "Data Narrative",
    "title": "XCP-ABCD (version 0.0.8)",
    "content": "Production Testing: . | 52/56 subjects completed XCP successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm1/fmriprep/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/xcp/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/xcp/output_ria | Path to xcp production audit: NA | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/xcp-derivatives/XCP | Path to xcp derivatives (concatenated): NA | . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#xcp-abcd-version-008",
    "relUrl": "/projects/fndm1/datanarrative/#xcp-abcd-version-008"
  },"17": {
    "doc": "Data Narrative",
    "title": "Post Processing",
    "content": ". | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | . To Do . | backup to PMACS | Add task events files | . ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/#post-processing",
    "relUrl": "/projects/fndm1/datanarrative/#post-processing"
  },"18": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/projects/fndm1/datanarrative/",
    "relUrl": "/projects/fndm1/datanarrative/"
  },"19": {
    "doc": "Data Narrative",
    "title": "Data Narrative for DAY2 - Margaret’s Curation",
    "content": "Data Processing Flow &amp; Important Links: . | Flow diagram that describes the lifecycle of this dataset curation and preprocessing may be viewed here | Overview: . | subjects without usable fmaps (no SDC run in fMRIPrep): ‘sub-12235’ ‘sub-13585’ ‘sub-14610’ ‘sub-14848’ ‘sub-14858’ ‘sub-14876’ ‘sub-15546’ ‘sub-16181’ ‘sub-16234’ ‘sub-17726’ *scans noted here | subjects/scans that failed fMRIPrep: sub-13373_ses-day2_task-face_run-01_bold.nii.gz, sub-14858_ses-day2_task-card_run-02_bold.nii.gz, sub-15709_ses-day2_task-rest_bold.nii.gz note: gzip error, can be rerun with later version | subjects/scans with poor QC: sub-15433 T1w (euler=782); sub-17378 card1, card2, face1, face2 (normCrossCorr &lt;0.8); sub-15276 face2 (normCrossCorr &lt;0.8); sub-15433 card 2 (normCrossCorr &lt;0.8); note: paths to XCP-generated .html reports for each subject and concatenated qc values provided below | . | DSR GitHub Project Page(Curation/Validation and Processing Queue Status): . | Cards for tracking the curation and validation portion of the dataset. This page should be updated every time you perform an action on the data. | Cards for tracking the progress of containerized pipeline runs on the data. | . | . Plan for the Data . | Why does PennLINC need this data? | For which project(s) is it intended? Please link to project pages below: | Goal is to curate and preprocess data | . Data Acquisition . | Data acquired by Dan Wolf | Describe the data: . | number of subjects = 125 | types of images = bold (2 runs task-face, 2 runs task-card, rest), T1w, T2w, DWI note: run1=task version A and run2 = task version B according to json SeriesDescription, see task-match.ipynb . | T1w = 125 subj | T2w = 3 subj | card_run-01 = 124 subj | card_run-02 = 124 subj | face_run-01 = 123 subj | face_run-02 = 124 subj | rest = 114 subj | fmap = 124 subj | dwi = 3 subj | . | . | . Download and Storage . | Data was stored as nifti files in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/day2. | Data was copied by Margaret to sub-project folder /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/original_data on 9/14/2021. | JSON’s within origial_data were updated using cubids-add-nifti-info. | Listing metadata fields using cubids-print-metadata-fields resulted: . | Acknowledgements | AcquisitionMatrixPE | AcquisitionNumber | Authors | BIDSVersion | BandwidthPerPixelPhaseEncode | BaseResolution | CoilString | ConversionSoftware | ConversionSoftwareVersion | DatasetDOI | DeidentificationMethod | DerivedVendorReportedEchoSpacing | DeviceSerialNumber | Dim1Size | Dim2Size | Dim3Size | DwellTime | EchoNumber | EchoTime | EchoTime1 | EchoTime2 | EchoTrainLength | EffectiveEchoSpacing | FlipAngle | Funding | HowToAcknowledge | ImageOrientationPatientDICOM | ImageType | ImagingFrequency | InPlanePhaseEncodingDirectionDICOM | IntendedFor | InversionTime | License | MRAcquisitionType | MagneticFieldStrength | Manufacturer | ManufacturersModelName | Modality | Name | NumVolumes | Obliquity | ParallelReductionFactorInPlane | PartialFourier | PercentPhaseFOV | PhaseEncodingDirection | PhaseEncodingSteps | PhaseResolution | PixelBandwidth | ProcedureStepDescription | ProtocolName | PulseSequenceDetails | ReceiveCoilName | ReconMatrixPE | RefLinesPE | ReferencesAndLinks | RepetitionTime | SAR | ScanOptions | ScanningSequence | SequenceName | SequenceVariant | SeriesDescription | SeriesInstanceUID | SeriesNumber | ShimSetting | SliceThickness | SliceTiming | SoftwareVersions | SpacingBetweenSlices | TaskName | TotalReadoutTime | TxRefAmp | VoxelSizeDim1 | VoxelSizeDim2 | VoxelSizeDim3 | template | . Running cubids-remove-metadata-fields resulted no PHI fields for removal. | Data checked into DataLad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS (dataset) via datalad save -m \"add initial data\" -d ./curation/BIDS action summary: add (ok: 2448) save (ok: 1) | . Curation Process . | Data curation by Margaret Gardner for NGG rotation on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration6 | . BIDS Validation: . | Iteration 1 (Ran cubids-validate and cubids-group simultanously as per The WAY, outputs saved to sandbox/validator_outputs/iteration1): EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 495 counts INCONSISTENT_SUBJECTS ( Not all subjects contain the same files. Each subject should contain the same number of files with the same naming unless some files are known to be missing. ) : 806 counts INCONSISTENT_PARAMETERS ( Not all subjects/sessions/runs have the same scanning parameters. ) : 24 subjects README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 subjects NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 subjects . | Iteration 1.2 (Reran cubids-validate with --ignore_nifti_headers and --ignore_subject_consistency, no modifications to datafiles): EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 495 scans README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 count NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 count . *counts using validator_err_counts.ipynb . | BIDS curation approved by Ted Satterthwaite and Tinashe Tapera on 9/21/21, last validator output of original data available at /gpfs/fs001/cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/validator_outputs/d2_r2_validation.csv. Data backed up to datalad. | . BIDS Optimization: . *NOTE: any files removed from curation/BIDS dataset noted in /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/curation_*_cmd.sh scripts, which are written by cubids-purge. Any files renamed (Acquisition Variants) noted in /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/apply*_cmd.sh scripts, which are written by cubids-apply. | BIDs groups from Iteration 1.2 reviewed by Ted and Tinashe . | reviewed subject files for duplicates, no subj with more than one T1w or each type of fmap (phase1, phase2, magnitude1, magnitude2) | . | 118 subj have full set of phase1&amp;2, magnitude1&amp;2 files | . | . | 5 subj have only phasediff files (mislabeled phase1) but no magnitude files | . | . | 1 subj has only phase1 &amp; phase2 files but no magnitude files | . | identified 3 subjects who have T2 data (KeyParamGroup=datatype-anat_suffix-T2w__1) in addition to T1 that compromise AcqGroup 3 | Plan to add A/B designation task entity for files to disambiguate task version (cardA,cardB, faceA, or faceB) performed during each run. Data currently contained in SeriesDescription, see task-match.ipynb *counts using validator_err_counts.ipynb * | . | Iteration 2 . | made sure all files in curation/BIDS checked into datalad | T2 files to be removed written to code/sandbox/T2w.txt using validator_err_counts.ipynb, ran cubids-purge: cubids-purge --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/T2w.txt | fmap files to be removed written to Margaret/Day2/curation/code/sandbox/validator_outputs/iteration1.2/fmap_to_rm.txt using validator_err_counts.ipynb, ran cubids-purge: cubids-purge --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/validator_outputs/iteration1.2/fmap_to_rm.txt | Reran cubids-validator iter2 with --ignore_nifti_headers and --ignore_subject_consistency flags; outputs identical to Iteration 1.2 above (reviewed using validator_parser.ipynb). | Reran cubids-group - still resulted in 23 acquisition groups, including addition of 4 new KeyParamGroups (reviewed using group_compare.ipynb): acquisition-VARIANTNoFmap_datatype-func_run-2_suffix-bold_task-card acquisition-VARIANTNoFmap_datatype-func_run-2_suffix-bold_task-face acquisition-VARIANTNoFmap_datatype-func_run-1_suffix-bold_task-face acquisition-VARIANTObliquityNoFmap_datatype-func_suffix-bold_task-rest acquisition-VARIANTNoFmap_datatype-func_suffix-bold_task-rest | Groupings approved by Ted and Tinashe, ran cubids-apply without modifications to iter2_summary or iter2_files: cubids-apply --use-datalad BIDS code/iterations/iteration2/iter2_summary.csv code/iterations/iteration2/iter2_files.csv code/iterations/apply1 | cubids-apply created apply1_full_cmd.sh (renamed to apply1a_full_cmd.sh) but unsuccessful in renaming files; internet disconnected and wasn’t able to copy error from jupyter terminal, reran command and reproduced error: raise CommandError( datalad.support.exceptions.CommandError: CommandError: 'bash code/iterations/apply1_full_cmd.sh' failed with exitcode 127 under /gpfs/fs001/cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS' | decided to edit iter2_summary.csv and rerun per Tinashe’s request to rename lengthy T1w keygroups, then will try to solve cubids-apply error . | renamed: . | KeyParamGroup datatype-anat_suffix-T1w__3 to acquisition-VARIANTAllwithParallelReductionFactorInPlane_datatype-anat_suffix-T1w | KeyParamGroup datatype-anat_suffix-T1w__4 to acquisition-VARIANTAll_datatype-anat_suffix-T1w Ran cubids-apply with above modifications to iter2_summary.csv: cubids-apply --use-datalad BIDS code/iterations/iteration2/iter2_summary.csv code/iterations/iteration2/iter2_files.csv code/iterations/apply2 | . | . | . | Iteration 3 . | per Sydney Covitz’s recommendations, reran cubids-group using full paths: cubids-group --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration3/iter3 | resulted in 23 acquisition groups, including addition of 4 new KeyParamGroups (reviewed using group_compare.ipynb): acquisition-VARIANTNumVolumesNoFmap_datatype-func_run-2_suffix-bold_task-face acquisition-VARIANTNumVolumesNoFmap_datatype-func_run-1_suffix-bold_task-face acquisition-VARIANTNumVolumesNoFmap_datatype-func_suffix-bold_task-rest acquisition-VARIANTNumVolumesNoFmap_datatype-func_suffix-bold_task-rest | reviewed with Sydney, discovered that prior cubids-apply attempts had succcessfully renamed IntendedFors field in fmap json’s but exited before being able to rename the filenames (due to the fact that the files.csv had the /gpfs/fs001/ string in it because cubids-group was run using relative paths), resulting in “NoFmap” additions above. Per Sydney’s recommendation running cubids-undo to un-rename IntendedFors; reran cubids-group and finally cubids-apply using abs. paths. | ran git clean -f -d to remove untracked changes in .ipynb_checkpoints | ran cubids-undo, used intendedfor_rename.ipynb to verify once VARIANT renames had been cleared. Datalad executes undone tracked below: . | HEAD is now at 69e473c Renamed IntendedFors | HEAD is now at 1ccd650 Renamed IntendedFors | HEAD is now at c8466c7 Renamed IntendedFors | HEAD is now at abc67c1 Renamed IntendedFors | HEAD is now at 26b23ee Renamed IntendedFors | HEAD is now at edfb983 updating .ipynb | . | . | . | Iteration 4 . | successfully removed all VARIANT intendedfors, rerunning: cubids-group --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration4/iter4 | reviewed groupings against iter2 using group_compare.ipynb, no changes. Renamed the lengthy T1w keygroups per Tinashe’s request: . | datatype-anat_suffix-T1w__3 : acquisition-VARIANTAllwithParallelReductionFactorInPlane_datatype-anat_suffix-T1w | datatype-anat_suffix-T1w__4 : acquisition-VARIANTAll_datatype-anat_suffix-T1w | . | ran: cubids-apply --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration4/iter4_summary.csv /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration4/iter4_files.csv /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/apply2 | cubids-apply successful | ran cubids-validate, no new errors or warnings: EVENTS_TSV_MISSING : 495 scans README_FILE_MISSING : 1 count NO_AUTHORS : 1 count | . | Iteration 5 . | 3 exemplar subjects (sub-15546, sub-16181, &amp; sub-12235) failed running fmriprep due to abberant image shape (64, 64, 43) in fmap images. Each subject compromised a unique Acquisition group. Deleting all fmap images (listed using Dim3_err_fmaps.ipynb): cubids-purge --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/fmap_to_rm2.txt | ran cubids-group, new groups ID’d for above subj (NoFMap) that will be merged into existing NoFMap groups with cubids-apply | ran cubids-apply without changes with prefix apply3, successful | ran cubids-validate, parsed using validator_parser.ipynb, no new errors or warnings: EVENTS_TSV_MISSING : 495 scans README_FILE_MISSING : 1 count NO_AUTHORS : 1 count | . | Iteration 6 . | 3 subjects (sub-13373, sub-14858, sub-15709) failed fmriprep due to CRC error, deleting nifti files identified in log outputs: cubids-purge --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/CRC_err_to_rm.txt *NOTE: these 3 scans can be rerun in the future with a different version of fmriprep that doesn’t have this gzip error! | ran cubids-group, no new variants (no RenameKeyGroups for non-fmap KeyGroups) - Tinashe reviewed, no need for cubids-apply/validate | . | Timing Files . | created event timing files (events.tsv) based on K23_fmri_paradigm.xls provided by Dan Wolf . | used stick files to create two .csv’s listing all events for run-1 (task A) and run-2 (task B) respectively (cardA and faceA have the same timings/outcome order, just the stimuli are different; cardB and faceB have the same timings/outcomes) | ONSET TIMES IN STICK FILES REFLECT FACT THAT ANALYSIS PIPELINE DELETED FIRST 20 SECONDS=10TR OF BOLD RUNS, DURING WHICH TWO “DUMMY” TASK TRIALS OCCURRED | converted to .tsv’s using csv_to_tsv.ipynb | . | copied .tsvs into Day2/curation/BIDS, reran cubids-validate; took several iterations of renaming events.tsv’s so they will be correctly applied/pass validator, succeded on iteration 5 (/cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/validator_outputs/tsv5_validation.csv) | . | . Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Margaret Gardner is responsible for running preprocessing pipelines/audits on CUBIC | Exemplar Testing: . | ran cubids-copy-exemplars --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/testing/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/apply2_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/testing/exemplars_dir | Path to fmriprep container (.sif copied from dropbox, annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/testing/exemplars_test/fmriprep-container | ran (tail -n 1 code/qsub_calls.sh) w/out modifications to participant_job.sh or fmriprep_zip.sh but no output branch created and didn’t save job number; reran, job writing to analysis/logs but seems unable to create new datalad branch (pushingitremote... line 32: datalad: command not found); ** edited participant_job.sh to correct conda environment (from base to margaret_reward) and run job in /cbica/comp_space; failed b/c had comments in-line on fmriprep_zip.sh ** reviewed with Tinashe and edited fmriprep_zip.sh; reran job 1424461 (“fpsub-12583”) has been submitted - completed successfully ** ran bash code/qsub_calls.sh, submitted jobs 1679260 through 1679282 &amp; merged to merge_ds (with help from Sydney &amp; Matt - issues with merge failing since test sub-12583 had already been merged, followed their instruction to delete both sub-12583 branches since .zip files already present in merge_ds) | error in sub-15546, sub-16181, &amp; sub-12235 (fmap images with Dim3Size=43, unable to construct fmaps - removing all fmap images for these subjects, see Iteration 5 above) *Note: flag --use-syn-sdc not included in fmriprep run call, so no susceptibility distortion correction was run for subjects without fieldmaps | sub-12583 (test sub) doesn’t have branch in output_ria but is fine in audit . | Path to exemplar outputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/testing/fmriprep/output_ria testing dir deleted to save space on CUBIC on 12/2/21, once production completed | . | . | Production Testing: . | ran qsub_calls.sh, submitted jobs 1831777 through 1831903 | only 84 files in logfile, 123 branches created under output_ria . | running merge_outputs.sh and fmriprep-audit to identify failed subj | edited concat_outputs.sh(still old version on github) to pull tinashe’s new concatenator.py edits and edited line 12 ‘concat_ds/csvs’ to ‘csvs’ | . | reviewed FMRIPREP_AUDIT.csv, 4 subj failed: . | sub-13373 - nipype.workflow ERROR: Node bold_to_std_transform.a0 failed to run on host 2119fmn002… File “indexed_gzip/indexed_gzip.pyx”, line 635, in indexed_gzip.indexed_gzip._IndexedGzipFile.seek indexed_gzip.indexed_gzip.CrcError: CRC/size validation failed - the GZIP data might be corrupt . | used gzip -t -v to validate CRC size for sub-13373_ses-day2_task-face_run-01_bold.nii.gz, OK | . | sub-14858 - same as above, err on sub-14858_ses-day2_task-card_acq-VARIANTNoFmap_run-02_bold.nii.gz | sub-15709 - same as above, err on sub-15709_ses-day2_task-rest_bold.nii.gz | sub-17113 - no error message, log o and e incomplete - to rerun | removing scans with CRC error, see Iteration 6 above - pushed BIDS updates to input_ria, rerunning qsub calls for sub-13373 (job 1974456), sub-14858 (job 1974459), sub-15709 (job 1974460), and sub-17113 (job 1974462); ran successfully based on logs, deleted merge_ds and reran merge_outputs.sh. Regot and reran fmriprep-audit | . | all 125 subjecs successfully processed | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/fmriprep/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/fmriprep/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/fmriprep-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/freesurfer-audit ** plotted Euler numbers generated by freesurfer_audit and plotted distribution. Sub-15433 recommended to be excluded from subsequent analyses (Euler=782). Reviewed sub-11305 (238) and sub-11399 (224) with Ted but ok’d | . | . | xcp-abcd . | Production Testing: . | edited participant_job.sh, xcp_zip.sh to update python environment, update “xcp-abcd-0-0-4” to “xcp-abcd-0-0-8” (matching container name and Tinashe’s scripts for other Reward projects) | ran test subject job 203853 (“xcpsub-17838”), successful! | submitted remaining jobs, successful! | submitted qsub_calls.sh for xcp-audit | wget and running bootstrap-quickunzip.sh to clone/unzip xcp outputs to xcp-derivatives; something didn’t work, seemed to overwrite unzip.sh? . | removed and wgot again, but had typo in path to xcp dir, rerunning with corrected path: qsub -cwd -N \"d2_unzip\" bootstrap-quickunzip.sh /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/xcp - job 213392 (“d2_unzip”) has been submitted; job didn’t seem to run, no outputs; see e and o output files. Rerunning from terminal (not qsub-ed), renamed dir wolf_satterthwaite_reward to derivatives-unzipped | concatenated *space-MNI152NLin6Asym_desc-qc_res-2_bold.csv outputs with xcp_qc_concat.ipynb, plotted and saved outputs to github dir qc_plots | . | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/fmriprep/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/xcp/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/xcp/output_ria | Path to xcp production audit: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/xcp-audit/XCP_AUDIT.csv | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/derivatives-unzipped/DERIVATIVES/XCP | Path to xcp derivatives (concatenated): /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/qc_d2.csv | . | . | . Post Processing . | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | FEAT task analysis . | fun side-quest for personal growth run by Margaret Gardner on CUBIC | wrote .txt timing files using fsl_timing_create.sh | Dan provided original feat analysis files for reference, saved under fsl_sandbox/dan_orig . | “the events folder has all the stickfiles, lots of different variations. the feat directory has a feat directory for this control participant’s cardA analysis: 11242_03360; the nifti images is that persons 4D bold timeseries used for that feat analysis.” | . | running on raw data from 3 subj randomly selected from Acquisition Group 1 (sub-16291, sub-15732, &amp; sub-15761) in /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/fsl_sandbox to familiarize with fsl workflow before adapting to accomodate fmriprep outputs; scripts run from git repo directory fsl . | ran BET on sub-15732 with default settings, pial surface not fully removed - reran with f=0.7 but removed too much, sticking with default f=0.5 | running FEAT preprocessing on sub-15732 card run-01: deleting 10 vol, set smoothing to 6.0 . | error in Registration: Could not find a supported file with prefix “/gpfs/fs001/cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/fsl_sandbox/BIDS/card_run-01.feat/example_func.nii.gz” | talked to Greer and discovered error was in bet outputting .hdr/.imgs instead of .nii.gz - need to define FSLOUTPUTTYPE=NIFTI_GZ. Removed all fsl outputs/reverting to raw BIDs to run again | . | ran BET on sub-15732 with default settings (-f 0.5), extraction looks good | ran FEAT preprocessing on sub-15732 card run-01: deleting 10 vol, set smoothing to 6.0; successful, | ran Stats on sub-15732 card run-1 with 4 EVs (cue, anticipation, win, lose) and 3 contrasts: (0, 0, 1, 0); (0, 0, 0, 1); (0, 0, 1, -1) | running full analyses on sub-15732 card run-1 with 4 EVs (cue, anticipation, win, lose) and 3 contrasts: (0, 0, 1, 0); (0, 0, 0, 1); (0, 0, 1, -1); successful, removed old feat directories (preprocessing/stats only) | duplicating/editing design.fsf to github, create seperate design.fsf’s for each task/run combo (starting with card1, potentially iterate across card/task in the future since they have identical EVs) | testing design_card1.fsf on sub-16291, ran successfully | created design files for each task/run and updated run_1stLevel_Analysis.sh | ran run_1stLevel_Analysis.sh note: outputs and QCs not reviewed since this was for an exercise - would review and/or rerun if intending to use outputs in the future . | removed sub-16291/card1+.feat dir | . | running 2nd level fixed-effects for card task (averaged across run 1 and 2 for each subj), output to fsl_sandbox/card_2ndLevel.gfeat | running 3rd level FLAME 1 model for card cope 3 (win-lose), default thresholds (cluster z=3.1, p=0.05); inputs ``fsl_sandbox/card_2ndLevel.gfeat/cope3.feat/stats/cope?.nii.gz; output to fsl_sandbox/card_3rdLevel_win-lose.gfeat` . | also ran uncorrected analysis to fsl_sandbox/card_3rdLevel_win-lose_uncorr.gfeat just for fun | . | . | . | . To Do . | backup to PMACS | rename task entity (1 and 2 vs A and B) | script group level task analyses in FEAT | bootstrap FEAT analyses for datalad | adapt feat script to accept fmriprep outputs | . ",
    "url": "http://localhost:4000/projects/day2/datanarrative/#data-narrative-for-day2---margarets-curation",
    "relUrl": "/projects/day2/datanarrative/#data-narrative-for-day2---margarets-curation"
  },"20": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/projects/day2/datanarrative/",
    "relUrl": "/projects/day2/datanarrative/"
  },"21": {
    "doc": "Data Narrative",
    "title": "Nodra",
    "content": ". | Data Processing Flow &amp; Important Links: | Plan for the Data | Data Acquisition | Download and Storage | Curation Process . | BIDS Validation: | BIDS Optimization: | . | Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Exemplar Testing: | Production Testing: | . | XCP-ABCD (version 0.0.8) . | Production Testing: | . | . | Post Processing . | To Do | . | . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#nodra",
    "relUrl": "/projects/nodra/datanarrative/#nodra"
  },"22": {
    "doc": "Data Narrative",
    "title": "Data Processing Flow &amp; Important Links:",
    "content": ". | Flow diagram that describes the lifecycle of this dataset curation and preprocessing: | . | DSR GitHub Project Page(Curation/Validation and Processing Queue Status): | . https://github.com/PennLINC/Reward/projects/1 . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#data-processing-flow--important-links",
    "relUrl": "/projects/nodra/datanarrative/#data-processing-flow--important-links"
  },"23": {
    "doc": "Data Narrative",
    "title": "Plan for the Data",
    "content": ". | Why does PennLINC need this data? Acquired at UPenn . | For which project(s) is it intended? Please link to project pages below: Reward Project . | Goal: Curate and preprocess an amalgam of datasets for a harmonized PennLINC resource . | . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#plan-for-the-data",
    "relUrl": "/projects/nodra/datanarrative/#plan-for-the-data"
  },"24": {
    "doc": "Data Narrative",
    "title": "Data Acquisition",
    "content": ". | Data acquired by Dan Wolf &amp; Ted Satterthwaite | Describe the data: . | number of subjects = 104 | types of images = bold (2 runs task-ITC, 2 runs task-CARD, rest), T1w, fieldmaps | . | . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#data-acquisition",
    "relUrl": "/projects/nodra/datanarrative/#data-acquisition"
  },"25": {
    "doc": "Data Narrative",
    "title": "Download and Storage",
    "content": ". | Original data available on Flywheel | Source data (NIfTI) on CUBIC in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/nodra. | Data was copied to /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/nodra and checked in to datalad after removing PHI (below) | JSON’s within origial_data were updated using cubids-add-nifti-info. | Listing metadata fields using cubids-print-metadata-fields gave the following fields: | . Acknowledgements AcquisitionDateTime AcquisitionMatrixPE AcquisitionNumber AcquisitionTime Authors BIDSVersion BandwidthPerPixelPhaseEncode BaseResolution CoilString ConversionSoftware ConversionSoftwareVersion DatasetDOI DeidentificationMethod DerivedVendorReportedEchoSpacing DeviceSerialNumber DwellTime EchoNumber EchoTime EchoTime1 EchoTime2 EchoTrainLength EffectiveEchoSpacing FlipAngle Funding HowToAcknowledge ImageOrientationPatientDICOM ImageType ImagingFrequency InPlanePhaseEncodingDirectionDICOM InstitutionAddress InstitutionName InstitutionalDepartmentName IntendedFor InversionTime License MRAcquisitionType MagneticFieldStrength Manufacturer ManufacturersModelName Modality Name ParallelReductionFactorInPlane PartialFourier PatientPosition PatientSex PercentPhaseFOV PhaseEncodingDirection PhaseEncodingSteps PhaseResolution PixelBandwidth ProcedureStepDescription ProtocolName PulseSequenceDetails ReceiveCoilName ReconMatrixPE RefLinesPE ReferencesAndLinks RepetitionTime SAR ScanOptions ScanningSequence SequenceName SequenceVariant SeriesDescription SeriesInstanceUID SeriesNumber ShimSetting SliceThickness SliceTiming SoftwareVersions SpacingBetweenSlices StationName StudyID StudyInstanceUID TaskName TotalReadoutTime TxRefAmp template ImageComments MultibandAccelerationFactor . Offending fields were removed with cubids-remove-metadata-fields in /cbica/projects/wolf_satterthwaite_reward/Curation/code/metadatafields_remove.sh . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#download-and-storage",
    "relUrl": "/projects/nodra/datanarrative/#download-and-storage"
  },"26": {
    "doc": "Data Narrative",
    "title": "Curation Process",
    "content": ". | Data curation by Tinashe Tapera on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iteration7/nodra/ | . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#curation-process",
    "relUrl": "/projects/nodra/datanarrative/#curation-process"
  },"27": {
    "doc": "Data Narrative",
    "title": "BIDS Validation:",
    "content": ". | Data with short bold time series (&lt;3mins) were removed with the Notebook /cbica/projects/wolf_satterthwaite_reward/Curation/code/RemoveShortBOLD.ipynb and cubids with /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/nodra/purge_low_bold.txt as input . | All validation outputs are available in chronological order in /cbica/projects/wolf_satterthwaite_reward/Curation/code/validate_outputs/nodra; most recent validation errors being: . | EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 394 counts . | README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 count . | NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 count . | . | . Data at this stage were approved for preprocessing. ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#bids-validation",
    "relUrl": "/projects/nodra/datanarrative/#bids-validation"
  },"28": {
    "doc": "Data Narrative",
    "title": "BIDS Optimization:",
    "content": ". | All cubids optimization results are available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter&lt;ITERATION_NUMBER&gt;/nodra/ . | Final optimization resulted in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter7/nodra/nodra_summary.csv . | . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#bids-optimization",
    "relUrl": "/projects/nodra/datanarrative/#bids-optimization"
  },"29": {
    "doc": "Data Narrative",
    "title": "Preprocessing Pipelines",
    "content": " ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#preprocessing-pipelines",
    "relUrl": "/projects/nodra/datanarrative/#preprocessing-pipelines"
  },"30": {
    "doc": "Data Narrative",
    "title": "fMRIPrep (version 20.2.3)",
    "content": ". | Tinashe Tapera was responsible for running preprocessing pipelines/audits on CUBIC | . Exemplar Testing: . | Used cubids to create exemplar dataset: cubids-copy-exemplars /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/nodra/BIDS /cbica/projects/wolf_satterthwaite_reward/Testing/nodra/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter6/nodra/nodra_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Testing/nodra/exemplars_dir | Path to fmriprep container: Original in ~/dropbox, datalad in /cbica/projects/wolf_satterthwaite_reward/Testing/exemplars_test/fmriprep-container . | Adjustments: . | During testing, some fieldmaps were found to be corrupt/unusable for the data, and were removed with cubids-purge. These files are in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/nodra/purge_fmaps.txt; likewise, one BOLD run was unusable (gzip error) and similarly removed with /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/nodra/purge_broken_card&lt;BBLID&gt;.txt | . | . Testing directory was deleted to save space on CUBIC on 12/2/21, once production completed . Production Testing: . | 103/104 subjects completed fMRIPrep successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/nodra/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/fmriprep/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/fmriprep/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/fmriprep/-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: NA | . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#fmriprep-version-2023",
    "relUrl": "/projects/nodra/datanarrative/#fmriprep-version-2023"
  },"31": {
    "doc": "Data Narrative",
    "title": "XCP-ABCD (version 0.0.8)",
    "content": "Production Testing: . | 103/104 subjects completed XCP successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/nodra/fmriprep/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/xcp/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/xcp/output_ria | Path to xcp production audit: NA | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/xcp-derivatives/XCP | Path to xcp derivatives (concatenated): NA | . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#xcp-abcd-version-008",
    "relUrl": "/projects/nodra/datanarrative/#xcp-abcd-version-008"
  },"32": {
    "doc": "Data Narrative",
    "title": "Post Processing",
    "content": ". | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | . To Do . | backup to PMACS | Add task events files | . ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/#post-processing",
    "relUrl": "/projects/nodra/datanarrative/#post-processing"
  },"33": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/projects/nodra/datanarrative/",
    "relUrl": "/projects/nodra/datanarrative/"
  },"34": {
    "doc": "Data Narrative",
    "title": "FNDM2",
    "content": ". | Data Processing Flow &amp; Important Links: | Plan for the Data | Data Acquisition | Download and Storage | Curation Process . | BIDS Validation: | BIDS Optimization: | . | Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Exemplar Testing: | Production Testing: | . | XCP-ABCD (version 0.0.8) . | Production Testing: | . | . | Post Processing . | To Do | . | . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#fndm2",
    "relUrl": "/projects/fndm2/datanarrative/#fndm2"
  },"35": {
    "doc": "Data Narrative",
    "title": "Data Processing Flow &amp; Important Links:",
    "content": ". | Flow diagram that describes the lifecycle of this dataset curation and preprocessing: | . | DSR GitHub Project Page(Curation/Validation and Processing Queue Status): | . https://github.com/PennLINC/Reward/projects/1 . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#data-processing-flow--important-links",
    "relUrl": "/projects/fndm2/datanarrative/#data-processing-flow--important-links"
  },"36": {
    "doc": "Data Narrative",
    "title": "Plan for the Data",
    "content": ". | Why does PennLINC need this data? Acquired at UPenn . | For which project(s) is it intended? Please link to project pages below: Reward Project . | Goal: Curate and preprocess an amalgam of datasets for a harmonized PennLINC resource . | . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#plan-for-the-data",
    "relUrl": "/projects/fndm2/datanarrative/#plan-for-the-data"
  },"37": {
    "doc": "Data Narrative",
    "title": "Data Acquisition",
    "content": ". | Data acquired by Dan Wolf &amp; Ted Satterthwaite | Describe the data: . | number of subjects = 103 | types of images = bold (4 runs task-ITC, rest), T1w, fieldmaps | . | . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#data-acquisition",
    "relUrl": "/projects/fndm2/datanarrative/#data-acquisition"
  },"38": {
    "doc": "Data Narrative",
    "title": "Download and Storage",
    "content": ". | Original data available on Flywheel | Source data (NIfTI) on CUBIC in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/fndm2. | Data was copied to /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm2 and checked in to datalad after removing PHI (below) | JSON’s within origial_data were updated using cubids-add-nifti-info. | Listing metadata fields using cubids-print-metadata-fields gave the following fields: | . Acknowledgements AcquisitionDateTime AcquisitionMatrixPE AcquisitionNumber AcquisitionTime Authors BIDSVersion BandwidthPerPixelPhaseEncode BaseResolution CoilString ConversionSoftware ConversionSoftwareVersion DatasetDOI DeidentificationMethod DerivedVendorReportedEchoSpacing DeviceSerialNumber DwellTime EchoNumber EchoTime EchoTime1 EchoTime2 EchoTrainLength EffectiveEchoSpacing FlipAngle Funding HowToAcknowledge ImageOrientationPatientDICOM ImageType ImagingFrequency InPlanePhaseEncodingDirectionDICOM InstitutionAddress InstitutionName InstitutionalDepartmentName IntendedFor InversionTime License MRAcquisitionType MagneticFieldStrength Manufacturer ManufacturersModelName Modality Name ParallelReductionFactorInPlane PartialFourier PatientPosition PatientSex PercentPhaseFOV PhaseEncodingDirection PhaseEncodingSteps PhaseResolution PixelBandwidth ProcedureStepDescription ProtocolName PulseSequenceDetails ReceiveCoilName ReconMatrixPE RefLinesPE ReferencesAndLinks RepetitionTime SAR ScanOptions ScanningSequence SequenceName SequenceVariant SeriesDescription SeriesInstanceUID SeriesNumber ShimSetting SliceThickness SliceTiming SoftwareVersions SpacingBetweenSlices StationName StudyID StudyInstanceUID TaskName TotalReadoutTime TxRefAmp template ImageComments MultibandAccelerationFactor . Offending fields were removed with cubids-remove-metadata-fields in /cbica/projects/wolf_satterthwaite_reward/Curation/code/metadatafields_remove.sh . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#download-and-storage",
    "relUrl": "/projects/fndm2/datanarrative/#download-and-storage"
  },"39": {
    "doc": "Data Narrative",
    "title": "Curation Process",
    "content": ". | Data curation by Tinashe Tapera on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iteration7/fndm2/ | . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#curation-process",
    "relUrl": "/projects/fndm2/datanarrative/#curation-process"
  },"40": {
    "doc": "Data Narrative",
    "title": "BIDS Validation:",
    "content": ". | Data with short bold time series (&lt;3mins) were removed with the Notebook /cbica/projects/wolf_satterthwaite_reward/Curation/code/RemoveShortBOLD.ipynb . | All validation outputs are available in chronological order in /cbica/projects/wolf_satterthwaite_reward/Curation/code/validate_outputs/fndm2; most recent validation errors being: . | EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 386 counts . | README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 count . | NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 count . | . | . Data at this stage were approved for preprocessing. ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#bids-validation",
    "relUrl": "/projects/fndm2/datanarrative/#bids-validation"
  },"41": {
    "doc": "Data Narrative",
    "title": "BIDS Optimization:",
    "content": ". | All cubids optimization results are available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter&lt;ITERATION_NUMBER&gt;/fndm2/ . | Final optimization resulted in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter6/fndm2/fndm2_summary.csv . | . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#bids-optimization",
    "relUrl": "/projects/fndm2/datanarrative/#bids-optimization"
  },"42": {
    "doc": "Data Narrative",
    "title": "Preprocessing Pipelines",
    "content": " ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#preprocessing-pipelines",
    "relUrl": "/projects/fndm2/datanarrative/#preprocessing-pipelines"
  },"43": {
    "doc": "Data Narrative",
    "title": "fMRIPrep (version 20.2.3)",
    "content": ". | Tinashe Tapera was responsible for running preprocessing pipelines/audits on CUBIC | . Exemplar Testing: . | Used cubids to create exemplar dataset: cubids-copy-exemplars /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm2/BIDS /cbica/projects/wolf_satterthwaite_reward/Testing/fndm2/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter6/fndm2/fndm2_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Testing/fndm2/exemplars_dir | Path to fmriprep container: Original in ~/dropbox, datalad in /cbica/projects/wolf_satterthwaite_reward/Testing/exemplars_test/fmriprep-container . | Adjustments: . | During testing, some fieldmaps were found to be corrupt/unusable for the data, and were removed. These files are in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/fndm2/purge_no_fmaps.txt | . | . Testing directory was deleted to save space on CUBIC on 12/2/21, once production completed . Production Testing: . | 102/103 subjects completed fMRIPrep successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm2/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/fmriprep/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/fmriprep/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/fmriprep/-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: NA | . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#fmriprep-version-2023",
    "relUrl": "/projects/fndm2/datanarrative/#fmriprep-version-2023"
  },"44": {
    "doc": "Data Narrative",
    "title": "XCP-ABCD (version 0.0.8)",
    "content": "Production Testing: . | 102/103 subjects completed XCP successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm2/fmriprep/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/xcp/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/xcp/output_ria | Path to xcp production audit: NA | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/xcp-derivatives/XCP | Path to xcp derivatives (concatenated): NA | . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#xcp-abcd-version-008",
    "relUrl": "/projects/fndm2/datanarrative/#xcp-abcd-version-008"
  },"45": {
    "doc": "Data Narrative",
    "title": "Post Processing",
    "content": ". | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | . To Do . | backup to PMACS | Add task events files | . ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/#post-processing",
    "relUrl": "/projects/fndm2/datanarrative/#post-processing"
  },"46": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/projects/fndm2/datanarrative/",
    "relUrl": "/projects/fndm2/datanarrative/"
  },"47": {
    "doc": "Data Narrative",
    "title": "CogTrain",
    "content": ". | Data Processing Flow &amp; Important Links: | Plan for the Data | Data Acquisition | Download and Storage | Curation Process . | BIDS Validation: | BIDS Optimization: | . | Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Exemplar Testing: | Production Testing: | . | XCP-ABCD (version 0.0.8) . | Production Testing: | . | . | Post Processing . | To Do | . | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#cogtrain",
    "relUrl": "/projects/cogtrain/datanarrative/#cogtrain"
  },"48": {
    "doc": "Data Narrative",
    "title": "Data Processing Flow &amp; Important Links:",
    "content": ". | Flow diagram that describes the lifecycle of this dataset curation and preprocessing: | . NA . | DSR GitHub Project Page(Curation/Validation and Processing Queue Status): | . https://github.com/PennLINC/Reward/projects/1 . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#data-processing-flow--important-links",
    "relUrl": "/projects/cogtrain/datanarrative/#data-processing-flow--important-links"
  },"49": {
    "doc": "Data Narrative",
    "title": "Plan for the Data",
    "content": ". | Why does PennLINC need this data? Acquired at UPenn . | For which project(s) is it intended? Please link to project pages below: Reward Project . | Goal: Curate and preprocess an amalgam of datasets for a harmonized PennLINC resource . | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#plan-for-the-data",
    "relUrl": "/projects/cogtrain/datanarrative/#plan-for-the-data"
  },"50": {
    "doc": "Data Narrative",
    "title": "Data Acquisition",
    "content": ". | Data acquired by Joe Kable | Describe the data: . | number of subjects = 166 | types of images = bold (4 runs task-RISK, 4 runs task-ITC, rest), T1w, fieldmaps, DWI | . | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#data-acquisition",
    "relUrl": "/projects/cogtrain/datanarrative/#data-acquisition"
  },"51": {
    "doc": "Data Narrative",
    "title": "Download and Storage",
    "content": ". | Original data available from Kable Lab | Source data (NIfTI) on CUBIC in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/CogTrain. | Data was copied to /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/CogTrain and checked in to datalad after removing PHI (below) | JSON’s within origial_data were updated using cubids-add-nifti-info. | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#download-and-storage",
    "relUrl": "/projects/cogtrain/datanarrative/#download-and-storage"
  },"52": {
    "doc": "Data Narrative",
    "title": "Curation Process",
    "content": ". | Data curation by Tinashe Tapera on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: NA | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#curation-process",
    "relUrl": "/projects/cogtrain/datanarrative/#curation-process"
  },"53": {
    "doc": "Data Narrative",
    "title": "BIDS Validation:",
    "content": ". | BIDS validation output at /cbica/projects/wolf_satterthwaite_reward/Curation/code/validate_outputs/CogTrain/CogTrain_validation_2.csv: . | INCONSISTENT_PARAMETERS ( Not all subjects/sessions/runs have the same scanning parameters. ) : 244 counts . | PARTICIPANT_ID_PATTERN (Participant_id column labels must consist of the pattern “sub-\".) : 166 . | . | . Data at this stage were approved for preprocessing. ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#bids-validation",
    "relUrl": "/projects/cogtrain/datanarrative/#bids-validation"
  },"54": {
    "doc": "Data Narrative",
    "title": "BIDS Optimization:",
    "content": ". | All cubids optimization results are available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter&lt;ITERATION_NUMBER&gt;/CogTrain/ . | Final optimization resulted in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter1/CogTrain/CogTrain_summary.csv . | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#bids-optimization",
    "relUrl": "/projects/cogtrain/datanarrative/#bids-optimization"
  },"55": {
    "doc": "Data Narrative",
    "title": "Preprocessing Pipelines",
    "content": " ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#preprocessing-pipelines",
    "relUrl": "/projects/cogtrain/datanarrative/#preprocessing-pipelines"
  },"56": {
    "doc": "Data Narrative",
    "title": "fMRIPrep (version 20.2.3)",
    "content": ". | Tinashe Tapera was responsible for running preprocessing pipelines/audits on CUBIC | . Exemplar Testing: . | Used cubids to create exemplar dataset: cubids-copy-exemplars /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/CogTrain/BIDS /cbica/projects/wolf_satterthwaite_reward/Testing/CogTrain/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter1/CogTrain/CogTrain_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Testing/CogTrain/exemplars_dir | Path to fmriprep container: Original in ~/dropbox, datalad in /cbica/projects/wolf_satterthwaite_reward/Testing/exemplars_test/fmriprep-container | . Testing directory was deleted to save space on CUBIC on 12/2/21, once production completed . Production Testing: . | 291/293 sessions completed fMRIPrep successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/CogTrain/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/fmriprep-multises/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/fmriprep-multises/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/fmriprep-multises-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: NA | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#fmriprep-version-2023",
    "relUrl": "/projects/cogtrain/datanarrative/#fmriprep-version-2023"
  },"57": {
    "doc": "Data Narrative",
    "title": "XCP-ABCD (version 0.0.8)",
    "content": "Production Testing: . | 289/293 sessions completed XCP successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/CogTrain/fmriprep-multises/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/xcp-multises/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/xcp-multises/output_ria | Path to xcp production audit: NA | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/xcp-derivatives/XCP | Path to xcp derivatives (concatenated): NA | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#xcp-abcd-version-008",
    "relUrl": "/projects/cogtrain/datanarrative/#xcp-abcd-version-008"
  },"58": {
    "doc": "Data Narrative",
    "title": "Post Processing",
    "content": ". | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | . To Do . | backup to PMACS | Add task events files | . ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/#post-processing",
    "relUrl": "/projects/cogtrain/datanarrative/#post-processing"
  },"59": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/projects/cogtrain/datanarrative/",
    "relUrl": "/projects/cogtrain/datanarrative/"
  },"60": {
    "doc": "Day2",
    "title": "Day2",
    "content": "This section outlines progress with the Day2 project. ",
    "url": "http://localhost:4000/projects/day2/day2_index/",
    "relUrl": "/projects/day2/day2_index/"
  },"61": {
    "doc": "FNDM1",
    "title": "FNDM1",
    "content": "This section outlines progress with the FNDM1 project. ",
    "url": "http://localhost:4000/projects/fndm1/fndm1_index/",
    "relUrl": "/projects/fndm1/fndm1_index/"
  },"62": {
    "doc": "FNDM2",
    "title": "FNDM2",
    "content": "This section outlines progress with the FNDM2 project. ",
    "url": "http://localhost:4000/projects/fndm2/fndm2_index/",
    "relUrl": "/projects/fndm2/fndm2_index/"
  },"63": {
    "doc": "Home",
    "title": "Reward Documentation",
    "content": "This site documents the progress of curation and preprocessing of the Reward datasets on CUBIC. This documentation continues on from Anna Xu’s curation on Flywheel (see here). ",
    "url": "http://localhost:4000/#reward-documentation",
    "relUrl": "/#reward-documentation"
  },"64": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },"65": {
    "doc": "ITC Task Events",
    "title": "Processing Task Event Files",
    "content": "In addition to preprocessing, we also attempted to parse ITC events files into BIDS valid format. The notebook with progress is available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/itc_eprime/parse_eprime.Rmd . ",
    "url": "http://localhost:4000/projects/cogtrain/itc_task_events/#processing-task-event-files",
    "relUrl": "/projects/cogtrain/itc_task_events/#processing-task-event-files"
  },"66": {
    "doc": "ITC Task Events",
    "title": "ITC Task Events",
    "content": " ",
    "url": "http://localhost:4000/projects/cogtrain/itc_task_events/",
    "relUrl": "/projects/cogtrain/itc_task_events/"
  },"67": {
    "doc": "NEFF V1 & V2",
    "title": "NEFF",
    "content": "This section outlines progress with the NEFF projects. ",
    "url": "http://localhost:4000/projects/neff/neff_index/#neff",
    "relUrl": "/projects/neff/neff_index/#neff"
  },"68": {
    "doc": "NEFF V1 & V2",
    "title": "NEFF V1 & V2",
    "content": " ",
    "url": "http://localhost:4000/projects/neff/neff_index/",
    "relUrl": "/projects/neff/neff_index/"
  },"69": {
    "doc": "Nodra",
    "title": "Nodra",
    "content": "This section outlines progress with the Nodra project. ",
    "url": "http://localhost:4000/projects/nodra/nodra_index/",
    "relUrl": "/projects/nodra/nodra_index/"
  }
}
