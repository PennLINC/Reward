{"0": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Reward Documentation (2020)",
    "content": "Last updated: July 1, 2020 Written by Anna Xu . Reward is a dataset comprised of 6 projects collected over the span of years, totaling 509 participants (note that some participants were present in multiple projects). These are the 6 projects with the scans available in them: . | FNDM1: T1w, ASL, b0 phase 1 and 2, b0 magnitude, resting-state, task fMRI (face run 1 and 2; card run 1 and 2) | FNDM2: T1w, b0 phase 1 and 2, b0 magnitude, resting-state, task fMRI (itc runs 1-4) | NEFF: T1w, b0 phasediff, b0 magnitude, task fMRI (effort runs 1-4) . | Note that 15 participants from FNDM2 have been reclassified into NEFF because they are neff pilots identified by Dan | . | NEFF2: T1w, b0 phasediff, b0 magnitude, task fMRI (effort runs 1-4) | NODRA: T1w, b0 phasediff, b0 magnitude, resting-state, task fMRI (card runs 1-2, itc runs 1-2) . | Note that some participants have multiband resting-state scans while others have single band or both | . | DAY2: T1w, ASL, b0 phase 1 and 2, b0 magnitude, resting-state, task fMRI (face run 1 and 2; card run 1 and 2), and DTI for very few participants (n&lt;20) | . ",
    "url": "http://localhost:4000/Reward/anna/#reward-documentation-2020",
    "relUrl": "/anna/#reward-documentation-2020"
  },"1": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Imaging &amp; Task Data Status (Flywheel)",
    "content": " ",
    "url": "http://localhost:4000/Reward/anna/#imaging--task-data-status-flywheel",
    "relUrl": "/anna/#imaging--task-data-status-flywheel"
  },"2": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Scan data &amp; associated task behavioral data",
    "content": "Completeness . All scan data found on XNAT and associated task behavioral data found on XNAT or CfN has been uploaded onto Flywheel in the project Reward2018. Scan data was de-identified by uploading with the command fw import dicom ${dicomUploadPath} mcieslak \"Reward2018\" --subject ${bblid} --session ${subproject} -y --profile \"dicom_config.yaml\" in the command line and with the configuration file dicom_config.yaml found on Flywheel in Reward2018 &gt; Information. The following procedure checked to make sure all expected scans on XNAT were accounted for on Flywheel: . | All participants on XNAT were cross-checked with all participants on Flywheel to ensure that each expected participant in Reward had their scans on Flywheel. | For each individual participant, completeness of scans were checked. If a participant did not have all the scans expected based on the project they were from, the missing scans were checked for on XNAT. If they existed on XNAT, the scans were uploaded onto Flywheel. If they were not on XNAT, they were documented on an excel file found on Flywheel in Reward2018 &gt; information &gt; missingScans_20200624.xls with any notes from XNAT documented. | This excel file contains the sheet missingScanNotes which details the bblid, project, scan missing, reason for scan missing (either not on xnat, upload error, or excluded participant), notes from xnat, and notes I’ve written. | The other sheet on this spreadsheet is missingScanCount which contains the number of scans missing for each particular scan in each project. | . | . Scanid for each participant has also been noted in the custom info for each subject/session. Fw-heudiconv . These scans have also been fw’heudiconv-ed using these heuristics and the command fw-heudiconv-curate --project \"Reward2018\" --heuristic \"${heuristicFile}\" --session \"${subproject}\" --subject \"${bblid}\". You can also use the script subsetFwheudiconv.sh to mass run fw-heudiconv for a select few participants. fMRIPrep . Some scans have been processed with an old version of fMRIPrep but all scans may need to be fMRIPrep’d again with the latest version of fMRIPrep. runfMRIPrep.py details the previous process of running fMRIPrep. Task files . Files documenting all nifti scans and their associated task behavioral data can be found on Flywheel in the file fwScansTask_20200624.csv. This file is located in Reward2018 &gt; Information and contains the following variables: . | bblid, projects | scan_file: name of the nifti | bids: BIDS name of the nifti | file_type: type of scan (e.g., b0 magnitude, resting-state, etc.) | assoc_task_file: name of task file uploaded onto Flywheel that is associated with the particular scan | is_task: tells you whether the scan is a task fMRI scan | is_task_missing: tells you whether the associated task file for a task fMRI scan is missing. | . Another file, missingTaskFiles_20200625.csv documents a summary of the missing task files. This file is currently on the reward_data_mgmt slack channel (unsure if this is also on the Reward2018 project on Flywheel due to inability to check Flywheel before my departure). This file contains the following: . | is_missing: tells you whether the participant is missing is missing a task file, but since this spreadsheet is just of participants missing a task behavioral file, they all are | n(): tells you how many task behavioral files associated with a participant is missing | . If needed, scanAuditFlywheel.py goes through the process of generating a large spreadsheet of all files present in the Reward2018 project. To generate these associated files, use the script scanTaskDocumentation.R which sources code from classify_scans.R and classify_task.R. ",
    "url": "http://localhost:4000/Reward/anna/#scan-data--associated-task-behavioral-data",
    "relUrl": "/anna/#scan-data--associated-task-behavioral-data"
  },"3": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Other data",
    "content": "Demographics, Medication Status, Diagnosis . Demographics, medication status, diagnosis, and effort data are currently with Dan (not uploaded anywhere). Missing data has been previously reported in the reward_data_mgmt slack channel. Aylin’s Monster . Aylin’s monster scripts can be found in the Reward GitHub repo by clicking here. A wiki page of old documentation for Aylin’s monster can be found here. ",
    "url": "http://localhost:4000/Reward/anna/#other-data",
    "relUrl": "/anna/#other-data"
  },"4": {
    "doc": "Legacy Curation (2018-2020)",
    "title": "Legacy Curation (2018-2020)",
    "content": " ",
    "url": "http://localhost:4000/Reward/anna/",
    "relUrl": "/anna/"
  },"5": {
    "doc": "CogTrain",
    "title": "CogTrain",
    "content": "This section outlines progress with the CogTrain project. ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/cogtrain_index/",
    "relUrl": "/projects/cogtrain/cogtrain_index/"
  },"6": {
    "doc": "Data Narrative",
    "title": "FNDM1",
    "content": ". | Data Processing Flow | Plan for the Data | Data Acquisition | Download and Storage | Curation Process . | BIDS Validation: | BIDS Optimization: | . | Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Exemplar Testing: | Production Testing: | . | XCP-ABCD (version 0.0.8) . | Production Testing: | . | . | Post Processing . | To Do | . | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#fndm1",
    "relUrl": "/projects/fndm1/datanarrative/#fndm1"
  },"7": {
    "doc": "Data Narrative",
    "title": "Data Processing Flow",
    "content": ". | Flow diagram that describes the lifecycle of this dataset curation and preprocessing: | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#data-processing-flow",
    "relUrl": "/projects/fndm1/datanarrative/#data-processing-flow"
  },"8": {
    "doc": "Data Narrative",
    "title": "Plan for the Data",
    "content": ". | Why does PennLINC need this data? Acquired at UPenn . | Goal: Curate and preprocess an amalgam of datasets for a harmonized PennLINC resource . | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#plan-for-the-data",
    "relUrl": "/projects/fndm1/datanarrative/#plan-for-the-data"
  },"9": {
    "doc": "Data Narrative",
    "title": "Data Acquisition",
    "content": ". | Data acquired by Dan Wolf &amp; Ted Satterthwaite | Describe the data: . | number of subjects = 56 | types of images = bold (2 runs task-CARD, 2 runs task-FACE, rest), T1w, fieldmaps | . | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#data-acquisition",
    "relUrl": "/projects/fndm1/datanarrative/#data-acquisition"
  },"10": {
    "doc": "Data Narrative",
    "title": "Download and Storage",
    "content": ". | Original data available on Flywheel | Source data (NIfTI) on CUBIC in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/fndm1. | Data was copied to /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm1 and checked in to datalad after removing PHI (below) | JSON’s within origial_data were updated using cubids-add-nifti-info. | Listing metadata fields using cubids-print-metadata-fields gave the following fields: | . Acknowledgements AcquisitionDateTime AcquisitionMatrixPE AcquisitionNumber AcquisitionTime Authors BIDSVersion BandwidthPerPixelPhaseEncode BaseResolution CoilString ConversionSoftware ConversionSoftwareVersion DatasetDOI DeidentificationMethod DerivedVendorReportedEchoSpacing DeviceSerialNumber DwellTime EchoNumber EchoTime EchoTime1 EchoTime2 EchoTrainLength EffectiveEchoSpacing FlipAngle Funding HowToAcknowledge ImageOrientationPatientDICOM ImageType ImagingFrequency InPlanePhaseEncodingDirectionDICOM InstitutionAddress InstitutionName InstitutionalDepartmentName IntendedFor InversionTime License MRAcquisitionType MagneticFieldStrength Manufacturer ManufacturersModelName Modality Name ParallelReductionFactorInPlane PartialFourier PatientPosition PatientSex PercentPhaseFOV PhaseEncodingDirection PhaseEncodingSteps PhaseResolution PixelBandwidth ProcedureStepDescription ProtocolName PulseSequenceDetails ReceiveCoilName ReconMatrixPE RefLinesPE ReferencesAndLinks RepetitionTime SAR ScanOptions ScanningSequence SequenceName SequenceVariant SeriesDescription SeriesInstanceUID SeriesNumber ShimSetting SliceThickness SliceTiming SoftwareVersions SpacingBetweenSlices StationName StudyID StudyInstanceUID TaskName TotalReadoutTime TxRefAmp template ImageComments MultibandAccelerationFactor . Offending fields were removed with cubids-remove-metadata-fields in /cbica/projects/wolf_satterthwaite_reward/Curation/code/metadatafields_remove.sh . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#download-and-storage",
    "relUrl": "/projects/fndm1/datanarrative/#download-and-storage"
  },"11": {
    "doc": "Data Narrative",
    "title": "Curation Process",
    "content": ". | Data curation by Tinashe Tapera on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iteration7/fndm1/ | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#curation-process",
    "relUrl": "/projects/fndm1/datanarrative/#curation-process"
  },"12": {
    "doc": "Data Narrative",
    "title": "BIDS Validation:",
    "content": ". | Data with short bold time series (&lt;3mins) were removed with the Notebook /cbica/projects/wolf_satterthwaite_reward/Curation/code/RemoveShortBOLD.ipynb . | All validation outputs are available in chronological order in /cbica/projects/wolf_satterthwaite_reward/Curation/code/validate_outputs/fndm1; most recent validation errors being: . | EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 224 counts . | README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 count . | NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 count . | . | . Data at this stage were approved for preprocessing. ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#bids-validation",
    "relUrl": "/projects/fndm1/datanarrative/#bids-validation"
  },"13": {
    "doc": "Data Narrative",
    "title": "BIDS Optimization:",
    "content": ". | All cubids optimization results are available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter&lt;ITERATION_NUMBER&gt;/fndm1/ . | Final optimization resulted in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter7/fndm1/fndm1_summary.csv . | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#bids-optimization",
    "relUrl": "/projects/fndm1/datanarrative/#bids-optimization"
  },"14": {
    "doc": "Data Narrative",
    "title": "Preprocessing Pipelines",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#preprocessing-pipelines",
    "relUrl": "/projects/fndm1/datanarrative/#preprocessing-pipelines"
  },"15": {
    "doc": "Data Narrative",
    "title": "fMRIPrep (version 20.2.3)",
    "content": ". | Tinashe Tapera was responsible for running preprocessing pipelines/audits on CUBIC | . Exemplar Testing: . | Used cubids to create exemplar dataset: cubids-copy-exemplars /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm1/BIDS /cbica/projects/wolf_satterthwaite_reward/Testing/fndm1/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter7/fndm1/fndm1_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Testing/fndm1/exemplars_dir | Path to fmriprep container: Original in ~/dropbox, datalad in /cbica/projects/wolf_satterthwaite_reward/Testing/exemplars_test/fmriprep-container . | Adjustments: . | During testing, some fieldmaps were found to be corrupt/unusable for the data, and were removed. These files are in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/fndm1/purge_broken_fmaps.txt | . | . Testing directory was deleted to save space on CUBIC on 12/2/21, once production completed . Production Testing: . | 52/56 subjects completed fMRIPrep successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm1/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/fmriprep/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/fmriprep/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/fmriprep/-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: NA | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#fmriprep-version-2023",
    "relUrl": "/projects/fndm1/datanarrative/#fmriprep-version-2023"
  },"16": {
    "doc": "Data Narrative",
    "title": "XCP-ABCD (version 0.0.8)",
    "content": "Production Testing: . | 52/56 subjects completed XCP successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm1/fmriprep/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/xcp/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/xcp/output_ria | Path to xcp production audit: NA | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Production/fndm1/xcp-derivatives/XCP | Path to xcp derivatives (concatenated): NA | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#xcp-abcd-version-008",
    "relUrl": "/projects/fndm1/datanarrative/#xcp-abcd-version-008"
  },"17": {
    "doc": "Data Narrative",
    "title": "Post Processing",
    "content": ". | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | . To Do . | backup to PMACS | Add task events files | . ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/#post-processing",
    "relUrl": "/projects/fndm1/datanarrative/#post-processing"
  },"18": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/fndm1/datanarrative/",
    "relUrl": "/projects/fndm1/datanarrative/"
  },"19": {
    "doc": "Data Narrative",
    "title": "Data Narrative for DAY2 - Margaret’s Curation",
    "content": "Data Processing Flow &amp; Important Links: . | Flow diagram that describes the lifecycle of this dataset curation and preprocessing may be viewed here | Overview: . | subjects without usable fmaps (no SDC run in fMRIPrep): ‘sub-12235’ ‘sub-13585’ ‘sub-14610’ ‘sub-14848’ ‘sub-14858’ ‘sub-14876’ ‘sub-15546’ ‘sub-16181’ ‘sub-16234’ ‘sub-17726’ *scans noted here | subjects/scans that failed fMRIPrep: sub-13373_ses-day2_task-face_run-01_bold.nii.gz, sub-14858_ses-day2_task-card_run-02_bold.nii.gz, sub-15709_ses-day2_task-rest_bold.nii.gz note: gzip error, can be rerun with later version | subjects/scans with poor QC: sub-15433 T1w (euler=782); sub-17378 card1, card2, face1, face2 (normCrossCorr &lt;0.8); sub-15276 face2 (normCrossCorr &lt;0.8); sub-15433 card 2 (normCrossCorr &lt;0.8); note: paths to XCP-generated .html reports for each subject and concatenated qc values provided below | . | . Plan for the Data . | Why does PennLINC need this data? | For which project(s) is it intended? Please link to project pages below: | Goal is to curate and preprocess data | . Data Acquisition . | Data acquired by Dan Wolf | Describe the data: . | number of subjects = 125 | types of images = bold (2 runs task-face, 2 runs task-card, rest), T1w, T2w, DWI note: run1=task version A and run2 = task version B according to json SeriesDescription, see task-match.ipynb . | T1w = 125 subj | T2w = 3 subj | card_run-01 = 124 subj | card_run-02 = 124 subj | face_run-01 = 123 subj | face_run-02 = 124 subj | rest = 114 subj | fmap = 124 subj | dwi = 3 subj | . | . | . Download and Storage . | Data was stored as nifti files in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/day2. | Data was copied by Margaret to sub-project folder /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/original_data on 9/14/2021. | JSON’s within origial_data were updated using cubids-add-nifti-info. | Listing metadata fields using cubids-print-metadata-fields resulted: . | Acknowledgements | AcquisitionMatrixPE | AcquisitionNumber | Authors | BIDSVersion | BandwidthPerPixelPhaseEncode | BaseResolution | CoilString | ConversionSoftware | ConversionSoftwareVersion | DatasetDOI | DeidentificationMethod | DerivedVendorReportedEchoSpacing | DeviceSerialNumber | Dim1Size | Dim2Size | Dim3Size | DwellTime | EchoNumber | EchoTime | EchoTime1 | EchoTime2 | EchoTrainLength | EffectiveEchoSpacing | FlipAngle | Funding | HowToAcknowledge | ImageOrientationPatientDICOM | ImageType | ImagingFrequency | InPlanePhaseEncodingDirectionDICOM | IntendedFor | InversionTime | License | MRAcquisitionType | MagneticFieldStrength | Manufacturer | ManufacturersModelName | Modality | Name | NumVolumes | Obliquity | ParallelReductionFactorInPlane | PartialFourier | PercentPhaseFOV | PhaseEncodingDirection | PhaseEncodingSteps | PhaseResolution | PixelBandwidth | ProcedureStepDescription | ProtocolName | PulseSequenceDetails | ReceiveCoilName | ReconMatrixPE | RefLinesPE | ReferencesAndLinks | RepetitionTime | SAR | ScanOptions | ScanningSequence | SequenceName | SequenceVariant | SeriesDescription | SeriesInstanceUID | SeriesNumber | ShimSetting | SliceThickness | SliceTiming | SoftwareVersions | SpacingBetweenSlices | TaskName | TotalReadoutTime | TxRefAmp | VoxelSizeDim1 | VoxelSizeDim2 | VoxelSizeDim3 | template | . Running cubids-remove-metadata-fields resulted no PHI fields for removal. | Data checked into DataLad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS (dataset) via datalad save -m \"add initial data\" -d ./curation/BIDS action summary: add (ok: 2448) save (ok: 1) | . Curation Process . | Data curation by Margaret Gardner for NGG rotation on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration6 | . BIDS Validation: . | Iteration 1 (Ran cubids-validate and cubids-group simultanously as per The WAY, outputs saved to sandbox/validator_outputs/iteration1): EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 495 counts INCONSISTENT_SUBJECTS ( Not all subjects contain the same files. Each subject should contain the same number of files with the same naming unless some files are known to be missing. ) : 806 counts INCONSISTENT_PARAMETERS ( Not all subjects/sessions/runs have the same scanning parameters. ) : 24 subjects README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 subjects NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 subjects . | Iteration 1.2 (Reran cubids-validate with --ignore_nifti_headers and --ignore_subject_consistency, no modifications to datafiles): EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 495 scans README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 count NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 count . *counts using validator_err_counts.ipynb . | BIDS curation approved by Ted Satterthwaite and Tinashe Tapera on 9/21/21, last validator output of original data available at /gpfs/fs001/cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/validator_outputs/d2_r2_validation.csv. Data backed up to datalad. | . BIDS Optimization: . *NOTE: any files removed from curation/BIDS dataset noted in /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/curation_*_cmd.sh scripts, which are written by cubids-purge. Any files renamed (Acquisition Variants) noted in /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/apply*_cmd.sh scripts, which are written by cubids-apply. | BIDs groups from Iteration 1.2 reviewed by Ted and Tinashe . | reviewed subject files for duplicates, no subj with more than one T1w or each type of fmap (phase1, phase2, magnitude1, magnitude2) | . | 118 subj have full set of phase1&amp;2, magnitude1&amp;2 files | . | . | 5 subj have only phasediff files (mislabeled phase1) but no magnitude files | . | . | 1 subj has only phase1 &amp; phase2 files but no magnitude files | . | identified 3 subjects who have T2 data (KeyParamGroup=datatype-anat_suffix-T2w__1) in addition to T1 that compromise AcqGroup 3 | Plan to add A/B designation task entity for files to disambiguate task version (cardA,cardB, faceA, or faceB) performed during each run. Data currently contained in SeriesDescription, see task-match.ipynb *counts using validator_err_counts.ipynb * | . | Iteration 2 . | made sure all files in curation/BIDS checked into datalad | T2 files to be removed written to code/sandbox/T2w.txt using validator_err_counts.ipynb, ran cubids-purge: cubids-purge --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/T2w.txt | fmap files to be removed written to Margaret/Day2/curation/code/sandbox/validator_outputs/iteration1.2/fmap_to_rm.txt using validator_err_counts.ipynb, ran cubids-purge: cubids-purge --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/validator_outputs/iteration1.2/fmap_to_rm.txt | Reran cubids-validator iter2 with --ignore_nifti_headers and --ignore_subject_consistency flags; outputs identical to Iteration 1.2 above (reviewed using validator_parser.ipynb). | Reran cubids-group - still resulted in 23 acquisition groups, including addition of 4 new KeyParamGroups (reviewed using group_compare.ipynb): acquisition-VARIANTNoFmap_datatype-func_run-2_suffix-bold_task-card acquisition-VARIANTNoFmap_datatype-func_run-2_suffix-bold_task-face acquisition-VARIANTNoFmap_datatype-func_run-1_suffix-bold_task-face acquisition-VARIANTObliquityNoFmap_datatype-func_suffix-bold_task-rest acquisition-VARIANTNoFmap_datatype-func_suffix-bold_task-rest | Groupings approved by Ted and Tinashe, ran cubids-apply without modifications to iter2_summary or iter2_files: cubids-apply --use-datalad BIDS code/iterations/iteration2/iter2_summary.csv code/iterations/iteration2/iter2_files.csv code/iterations/apply1 | cubids-apply created apply1_full_cmd.sh (renamed to apply1a_full_cmd.sh) but unsuccessful in renaming files; internet disconnected and wasn’t able to copy error from jupyter terminal, reran command and reproduced error: raise CommandError( datalad.support.exceptions.CommandError: CommandError: 'bash code/iterations/apply1_full_cmd.sh' failed with exitcode 127 under /gpfs/fs001/cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS' | decided to edit iter2_summary.csv and rerun per Tinashe’s request to rename lengthy T1w keygroups, then will try to solve cubids-apply error . | renamed: . | KeyParamGroup datatype-anat_suffix-T1w__3 to acquisition-VARIANTAllwithParallelReductionFactorInPlane_datatype-anat_suffix-T1w | KeyParamGroup datatype-anat_suffix-T1w__4 to acquisition-VARIANTAll_datatype-anat_suffix-T1w Ran cubids-apply with above modifications to iter2_summary.csv: cubids-apply --use-datalad BIDS code/iterations/iteration2/iter2_summary.csv code/iterations/iteration2/iter2_files.csv code/iterations/apply2 | . | . | . | Iteration 3 . | per Sydney Covitz’s recommendations, reran cubids-group using full paths: cubids-group --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration3/iter3 | resulted in 23 acquisition groups, including addition of 4 new KeyParamGroups (reviewed using group_compare.ipynb): acquisition-VARIANTNumVolumesNoFmap_datatype-func_run-2_suffix-bold_task-face acquisition-VARIANTNumVolumesNoFmap_datatype-func_run-1_suffix-bold_task-face acquisition-VARIANTNumVolumesNoFmap_datatype-func_suffix-bold_task-rest acquisition-VARIANTNumVolumesNoFmap_datatype-func_suffix-bold_task-rest | reviewed with Sydney, discovered that prior cubids-apply attempts had succcessfully renamed IntendedFors field in fmap json’s but exited before being able to rename the filenames (due to the fact that the files.csv had the /gpfs/fs001/ string in it because cubids-group was run using relative paths), resulting in “NoFmap” additions above. Per Sydney’s recommendation running cubids-undo to un-rename IntendedFors; reran cubids-group and finally cubids-apply using abs. paths. | ran git clean -f -d to remove untracked changes in .ipynb_checkpoints | ran cubids-undo, used intendedfor_rename.ipynb to verify once VARIANT renames had been cleared. Datalad executes undone tracked below: . | HEAD is now at 69e473c Renamed IntendedFors | HEAD is now at 1ccd650 Renamed IntendedFors | HEAD is now at c8466c7 Renamed IntendedFors | HEAD is now at abc67c1 Renamed IntendedFors | HEAD is now at 26b23ee Renamed IntendedFors | HEAD is now at edfb983 updating .ipynb | . | . | . | Iteration 4 . | successfully removed all VARIANT intendedfors, rerunning: cubids-group --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration4/iter4 | reviewed groupings against iter2 using group_compare.ipynb, no changes. Renamed the lengthy T1w keygroups per Tinashe’s request: . | datatype-anat_suffix-T1w__3 : acquisition-VARIANTAllwithParallelReductionFactorInPlane_datatype-anat_suffix-T1w | datatype-anat_suffix-T1w__4 : acquisition-VARIANTAll_datatype-anat_suffix-T1w | . | ran: cubids-apply --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration4/iter4_summary.csv /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/iteration4/iter4_files.csv /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/apply2 | cubids-apply successful | ran cubids-validate, no new errors or warnings: EVENTS_TSV_MISSING : 495 scans README_FILE_MISSING : 1 count NO_AUTHORS : 1 count | . | Iteration 5 . | 3 exemplar subjects (sub-15546, sub-16181, &amp; sub-12235) failed running fmriprep due to abberant image shape (64, 64, 43) in fmap images. Each subject compromised a unique Acquisition group. Deleting all fmap images (listed using Dim3_err_fmaps.ipynb): cubids-purge --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/fmap_to_rm2.txt | ran cubids-group, new groups ID’d for above subj (NoFMap) that will be merged into existing NoFMap groups with cubids-apply | ran cubids-apply without changes with prefix apply3, successful | ran cubids-validate, parsed using validator_parser.ipynb, no new errors or warnings: EVENTS_TSV_MISSING : 495 scans README_FILE_MISSING : 1 count NO_AUTHORS : 1 count | . | Iteration 6 . | 3 subjects (sub-13373, sub-14858, sub-15709) failed fmriprep due to CRC error, deleting nifti files identified in log outputs: cubids-purge --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/CRC_err_to_rm.txt *NOTE: these 3 scans can be rerun in the future with a different version of fmriprep that doesn’t have this gzip error! | ran cubids-group, no new variants (no RenameKeyGroups for non-fmap KeyGroups) - Tinashe reviewed, no need for cubids-apply/validate | . | Timing Files . | created event timing files (events.tsv) based on K23_fmri_paradigm.xls provided by Dan Wolf . | used stick files to create two .csv’s listing all events for run-1 (task A) and run-2 (task B) respectively (cardA and faceA have the same timings/outcome order, just the stimuli are different; cardB and faceB have the same timings/outcomes) | ONSET TIMES IN STICK FILES REFLECT FACT THAT ANALYSIS PIPELINE DELETED FIRST 20 SECONDS=10TR OF BOLD RUNS, DURING WHICH TWO “DUMMY” TASK TRIALS OCCURRED | converted to .tsv’s using csv_to_tsv.ipynb | . | copied .tsvs into Day2/curation/BIDS, reran cubids-validate; took several iterations of renaming events.tsv’s so they will be correctly applied/pass validator, succeded on iteration 5 (/cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/validator_outputs/tsv5_validation.csv) | . | . Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Margaret Gardner is responsible for running preprocessing pipelines/audits on CUBIC | Exemplar Testing: . | ran cubids-copy-exemplars --use-datalad /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/testing/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/iterations/apply2_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/testing/exemplars_dir | Path to fmriprep container (.sif copied from dropbox, annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/testing/exemplars_test/fmriprep-container | ran (tail -n 1 code/qsub_calls.sh) w/out modifications to participant_job.sh or fmriprep_zip.sh but no output branch created and didn’t save job number; reran, job writing to analysis/logs but seems unable to create new datalad branch (pushingitremote... line 32: datalad: command not found); ** edited participant_job.sh to correct conda environment (from base to margaret_reward) and run job in /cbica/comp_space; failed b/c had comments in-line on fmriprep_zip.sh ** reviewed with Tinashe and edited fmriprep_zip.sh; reran job 1424461 (“fpsub-12583”) has been submitted - completed successfully ** ran bash code/qsub_calls.sh, submitted jobs 1679260 through 1679282 &amp; merged to merge_ds (with help from Sydney &amp; Matt - issues with merge failing since test sub-12583 had already been merged, followed their instruction to delete both sub-12583 branches since .zip files already present in merge_ds) | error in sub-15546, sub-16181, &amp; sub-12235 (fmap images with Dim3Size=43, unable to construct fmaps - removing all fmap images for these subjects, see Iteration 5 above) *Note: flag --use-syn-sdc not included in fmriprep run call, so no susceptibility distortion correction was run for subjects without fieldmaps | sub-12583 (test sub) doesn’t have branch in output_ria but is fine in audit . | Path to exemplar outputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/testing/fmriprep/output_ria testing dir deleted to save space on CUBIC on 12/2/21, once production completed | . | . | Production Testing: . | ran qsub_calls.sh, submitted jobs 1831777 through 1831903 | only 84 files in logfile, 123 branches created under output_ria . | running merge_outputs.sh and fmriprep-audit to identify failed subj | edited concat_outputs.sh(still old version on github) to pull tinashe’s new concatenator.py edits and edited line 12 ‘concat_ds/csvs’ to ‘csvs’ | . | reviewed FMRIPREP_AUDIT.csv, 4 subj failed: . | sub-13373 - nipype.workflow ERROR: Node bold_to_std_transform.a0 failed to run on host 2119fmn002… File “indexed_gzip/indexed_gzip.pyx”, line 635, in indexed_gzip.indexed_gzip._IndexedGzipFile.seek indexed_gzip.indexed_gzip.CrcError: CRC/size validation failed - the GZIP data might be corrupt . | used gzip -t -v to validate CRC size for sub-13373_ses-day2_task-face_run-01_bold.nii.gz, OK | . | sub-14858 - same as above, err on sub-14858_ses-day2_task-card_acq-VARIANTNoFmap_run-02_bold.nii.gz | sub-15709 - same as above, err on sub-15709_ses-day2_task-rest_bold.nii.gz | sub-17113 - no error message, log o and e incomplete - to rerun | removing scans with CRC error, see Iteration 6 above - pushed BIDS updates to input_ria, rerunning qsub calls for sub-13373 (job 1974456), sub-14858 (job 1974459), sub-15709 (job 1974460), and sub-17113 (job 1974462); ran successfully based on logs, deleted merge_ds and reran merge_outputs.sh. Regot and reran fmriprep-audit | . | all 125 subjecs successfully processed | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/fmriprep/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/fmriprep/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/fmriprep-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/freesurfer-audit ** plotted Euler numbers generated by freesurfer_audit and plotted distribution. Sub-15433 recommended to be excluded from subsequent analyses (Euler=782). Reviewed sub-11305 (238) and sub-11399 (224) with Ted but ok’d | . | . | xcp-abcd . | Production Testing: . | edited participant_job.sh, xcp_zip.sh to update python environment, update “xcp-abcd-0-0-4” to “xcp-abcd-0-0-8” (matching container name and Tinashe’s scripts for other Reward projects) | ran test subject job 203853 (“xcpsub-17838”), successful! | submitted remaining jobs, successful! | submitted qsub_calls.sh for xcp-audit | wget and running bootstrap-quickunzip.sh to clone/unzip xcp outputs to xcp-derivatives; something didn’t work, seemed to overwrite unzip.sh? . | removed and wgot again, but had typo in path to xcp dir, rerunning with corrected path: qsub -cwd -N \"d2_unzip\" bootstrap-quickunzip.sh /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/xcp - job 213392 (“d2_unzip”) has been submitted; job didn’t seem to run, no outputs; see e and o output files. Rerunning from terminal (not qsub-ed), renamed dir wolf_satterthwaite_reward to derivatives-unzipped | concatenated *space-MNI152NLin6Asym_desc-qc_res-2_bold.csv outputs with xcp_qc_concat.ipynb, plotted and saved outputs to github dir qc_plots | . | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/fmriprep/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/xcp/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/xcp/output_ria | Path to xcp production audit: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/xcp-audit/XCP_AUDIT.csv | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/production/derivatives-unzipped/DERIVATIVES/XCP | Path to xcp derivatives (concatenated): /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/curation/code/sandbox/qc_d2.csv | . | . | . Post Processing . | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | FEAT task analysis . | fun side-quest for personal growth run by Margaret Gardner on CUBIC | wrote .txt timing files using fsl_timing_create.sh | Dan provided original feat analysis files for reference, saved under fsl_sandbox/dan_orig . | “the events folder has all the stickfiles, lots of different variations. the feat directory has a feat directory for this control participant’s cardA analysis: 11242_03360; the nifti images is that persons 4D bold timeseries used for that feat analysis.” | . | running on raw data from 3 subj randomly selected from Acquisition Group 1 (sub-16291, sub-15732, &amp; sub-15761) in /cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/fsl_sandbox to familiarize with fsl workflow before adapting to accomodate fmriprep outputs; scripts run from git repo directory fsl . | ran BET on sub-15732 with default settings, pial surface not fully removed - reran with f=0.7 but removed too much, sticking with default f=0.5 | running FEAT preprocessing on sub-15732 card run-01: deleting 10 vol, set smoothing to 6.0 . | error in Registration: Could not find a supported file with prefix “/gpfs/fs001/cbica/projects/wolf_satterthwaite_reward/Margaret/Day2/fsl_sandbox/BIDS/card_run-01.feat/example_func.nii.gz” | talked to Greer and discovered error was in bet outputting .hdr/.imgs instead of .nii.gz - need to define FSLOUTPUTTYPE=NIFTI_GZ. Removed all fsl outputs/reverting to raw BIDs to run again | . | ran BET on sub-15732 with default settings (-f 0.5), extraction looks good | ran FEAT preprocessing on sub-15732 card run-01: deleting 10 vol, set smoothing to 6.0; successful, | ran Stats on sub-15732 card run-1 with 4 EVs (cue, anticipation, win, lose) and 3 contrasts: (0, 0, 1, 0); (0, 0, 0, 1); (0, 0, 1, -1) | running full analyses on sub-15732 card run-1 with 4 EVs (cue, anticipation, win, lose) and 3 contrasts: (0, 0, 1, 0); (0, 0, 0, 1); (0, 0, 1, -1); successful, removed old feat directories (preprocessing/stats only) | duplicating/editing design.fsf to github, create seperate design.fsf’s for each task/run combo (starting with card1, potentially iterate across card/task in the future since they have identical EVs) | testing design_card1.fsf on sub-16291, ran successfully | created design files for each task/run and updated run_1stLevel_Analysis.sh | ran run_1stLevel_Analysis.sh note: outputs and QCs not reviewed since this was for an exercise - would review and/or rerun if intending to use outputs in the future . | removed sub-16291/card1+.feat dir | . | running 2nd level fixed-effects for card task (averaged across run 1 and 2 for each subj), output to fsl_sandbox/card_2ndLevel.gfeat | running 3rd level FLAME 1 model for card cope 3 (win-lose), default thresholds (cluster z=3.1, p=0.05); inputs ``fsl_sandbox/card_2ndLevel.gfeat/cope3.feat/stats/cope?.nii.gz; output to fsl_sandbox/card_3rdLevel_win-lose.gfeat` . | also ran uncorrected analysis to fsl_sandbox/card_3rdLevel_win-lose_uncorr.gfeat just for fun | . | . | . | . To Do . | backup to PMACS | rename task entity (1 and 2 vs A and B) | script group level task analyses in FEAT | bootstrap FEAT analyses for datalad | adapt feat script to accept fmriprep outputs | . ",
    "url": "http://localhost:4000/Reward/projects/day2/datanarrative/#data-narrative-for-day2---margarets-curation",
    "relUrl": "/projects/day2/datanarrative/#data-narrative-for-day2---margarets-curation"
  },"20": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/day2/datanarrative/",
    "relUrl": "/projects/day2/datanarrative/"
  },"21": {
    "doc": "Data Narrative",
    "title": "Nodra",
    "content": ". | Data Processing Flow &amp; Important Links: | Plan for the Data | Data Acquisition | Download and Storage | Curation Process . | BIDS Validation: | BIDS Optimization: | . | Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Exemplar Testing: | Production Testing: | . | XCP-ABCD (version 0.0.8) . | Production Testing: | . | . | Post Processing . | To Do | . | . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#nodra",
    "relUrl": "/projects/nodra/datanarrative/#nodra"
  },"22": {
    "doc": "Data Narrative",
    "title": "Data Processing Flow &amp; Important Links:",
    "content": ". | Flow diagram that describes the lifecycle of this dataset curation and preprocessing: | . | DSR GitHub Project Page(Curation/Validation and Processing Queue Status): | . https://github.com/PennLINC/Reward/projects/1 . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#data-processing-flow--important-links",
    "relUrl": "/projects/nodra/datanarrative/#data-processing-flow--important-links"
  },"23": {
    "doc": "Data Narrative",
    "title": "Plan for the Data",
    "content": ". | Why does PennLINC need this data? Acquired at UPenn . | For which project(s) is it intended? Please link to project pages below: Reward Project . | Goal: Curate and preprocess an amalgam of datasets for a harmonized PennLINC resource . | . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#plan-for-the-data",
    "relUrl": "/projects/nodra/datanarrative/#plan-for-the-data"
  },"24": {
    "doc": "Data Narrative",
    "title": "Data Acquisition",
    "content": ". | Data acquired by Dan Wolf &amp; Ted Satterthwaite | Describe the data: . | number of subjects = 104 | types of images = bold (2 runs task-ITC, 2 runs task-CARD, rest), T1w, fieldmaps | . | . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#data-acquisition",
    "relUrl": "/projects/nodra/datanarrative/#data-acquisition"
  },"25": {
    "doc": "Data Narrative",
    "title": "Download and Storage",
    "content": ". | Original data available on Flywheel | Source data (NIfTI) on CUBIC in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/nodra. | Data was copied to /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/nodra and checked in to datalad after removing PHI (below) | JSON’s within origial_data were updated using cubids-add-nifti-info. | Listing metadata fields using cubids-print-metadata-fields gave the following fields: | . Acknowledgements AcquisitionDateTime AcquisitionMatrixPE AcquisitionNumber AcquisitionTime Authors BIDSVersion BandwidthPerPixelPhaseEncode BaseResolution CoilString ConversionSoftware ConversionSoftwareVersion DatasetDOI DeidentificationMethod DerivedVendorReportedEchoSpacing DeviceSerialNumber DwellTime EchoNumber EchoTime EchoTime1 EchoTime2 EchoTrainLength EffectiveEchoSpacing FlipAngle Funding HowToAcknowledge ImageOrientationPatientDICOM ImageType ImagingFrequency InPlanePhaseEncodingDirectionDICOM InstitutionAddress InstitutionName InstitutionalDepartmentName IntendedFor InversionTime License MRAcquisitionType MagneticFieldStrength Manufacturer ManufacturersModelName Modality Name ParallelReductionFactorInPlane PartialFourier PatientPosition PatientSex PercentPhaseFOV PhaseEncodingDirection PhaseEncodingSteps PhaseResolution PixelBandwidth ProcedureStepDescription ProtocolName PulseSequenceDetails ReceiveCoilName ReconMatrixPE RefLinesPE ReferencesAndLinks RepetitionTime SAR ScanOptions ScanningSequence SequenceName SequenceVariant SeriesDescription SeriesInstanceUID SeriesNumber ShimSetting SliceThickness SliceTiming SoftwareVersions SpacingBetweenSlices StationName StudyID StudyInstanceUID TaskName TotalReadoutTime TxRefAmp template ImageComments MultibandAccelerationFactor . Offending fields were removed with cubids-remove-metadata-fields in /cbica/projects/wolf_satterthwaite_reward/Curation/code/metadatafields_remove.sh . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#download-and-storage",
    "relUrl": "/projects/nodra/datanarrative/#download-and-storage"
  },"26": {
    "doc": "Data Narrative",
    "title": "Curation Process",
    "content": ". | Data curation by Tinashe Tapera on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iteration7/nodra/ | . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#curation-process",
    "relUrl": "/projects/nodra/datanarrative/#curation-process"
  },"27": {
    "doc": "Data Narrative",
    "title": "BIDS Validation:",
    "content": ". | Data with short bold time series (&lt;3mins) were removed with the Notebook /cbica/projects/wolf_satterthwaite_reward/Curation/code/RemoveShortBOLD.ipynb and cubids with /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/nodra/purge_low_bold.txt as input . | All validation outputs are available in chronological order in /cbica/projects/wolf_satterthwaite_reward/Curation/code/validate_outputs/nodra; most recent validation errors being: . | EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 394 counts . | README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 count . | NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 count . | . | . Data at this stage were approved for preprocessing. ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#bids-validation",
    "relUrl": "/projects/nodra/datanarrative/#bids-validation"
  },"28": {
    "doc": "Data Narrative",
    "title": "BIDS Optimization:",
    "content": ". | All cubids optimization results are available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter&lt;ITERATION_NUMBER&gt;/nodra/ . | Final optimization resulted in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter7/nodra/nodra_summary.csv . | . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#bids-optimization",
    "relUrl": "/projects/nodra/datanarrative/#bids-optimization"
  },"29": {
    "doc": "Data Narrative",
    "title": "Preprocessing Pipelines",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#preprocessing-pipelines",
    "relUrl": "/projects/nodra/datanarrative/#preprocessing-pipelines"
  },"30": {
    "doc": "Data Narrative",
    "title": "fMRIPrep (version 20.2.3)",
    "content": ". | Tinashe Tapera was responsible for running preprocessing pipelines/audits on CUBIC | . Exemplar Testing: . | Used cubids to create exemplar dataset: cubids-copy-exemplars /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/nodra/BIDS /cbica/projects/wolf_satterthwaite_reward/Testing/nodra/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter6/nodra/nodra_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Testing/nodra/exemplars_dir | Path to fmriprep container: Original in ~/dropbox, datalad in /cbica/projects/wolf_satterthwaite_reward/Testing/exemplars_test/fmriprep-container . | Adjustments: . | During testing, some fieldmaps were found to be corrupt/unusable for the data, and were removed with cubids-purge. These files are in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/nodra/purge_fmaps.txt; likewise, one BOLD run was unusable (gzip error) and similarly removed with /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/nodra/purge_broken_card&lt;BBLID&gt;.txt | . | . Testing directory was deleted to save space on CUBIC on 12/2/21, once production completed . Production Testing: . | 103/104 subjects completed fMRIPrep successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/nodra/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/fmriprep/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/fmriprep/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/fmriprep/-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: NA | . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#fmriprep-version-2023",
    "relUrl": "/projects/nodra/datanarrative/#fmriprep-version-2023"
  },"31": {
    "doc": "Data Narrative",
    "title": "XCP-ABCD (version 0.0.8)",
    "content": "Production Testing: . | 103/104 subjects completed XCP successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/nodra/fmriprep/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/xcp/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/xcp/output_ria | Path to xcp production audit: NA | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Production/nodra/xcp-derivatives/XCP | Path to xcp derivatives (concatenated): NA | . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#xcp-abcd-version-008",
    "relUrl": "/projects/nodra/datanarrative/#xcp-abcd-version-008"
  },"32": {
    "doc": "Data Narrative",
    "title": "Post Processing",
    "content": ". | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | . To Do . | backup to PMACS | Add task events files | . ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/#post-processing",
    "relUrl": "/projects/nodra/datanarrative/#post-processing"
  },"33": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/nodra/datanarrative/",
    "relUrl": "/projects/nodra/datanarrative/"
  },"34": {
    "doc": "Data Narrative",
    "title": "FNDM2",
    "content": ". | Data Processing Flow &amp; Important Links: | Plan for the Data | Data Acquisition | Download and Storage | Curation Process . | BIDS Validation: | BIDS Optimization: | . | Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Exemplar Testing: | Production Testing: | . | XCP-ABCD (version 0.0.8) . | Production Testing: | . | . | Post Processing . | To Do | . | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#fndm2",
    "relUrl": "/projects/fndm2/datanarrative/#fndm2"
  },"35": {
    "doc": "Data Narrative",
    "title": "Data Processing Flow &amp; Important Links:",
    "content": ". | Flow diagram that describes the lifecycle of this dataset curation and preprocessing: | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#data-processing-flow--important-links",
    "relUrl": "/projects/fndm2/datanarrative/#data-processing-flow--important-links"
  },"36": {
    "doc": "Data Narrative",
    "title": "Plan for the Data",
    "content": ". | Why does PennLINC need this data? Acquired at UPenn . | Goal: Curate and preprocess an amalgam of datasets for a harmonized PennLINC resource . | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#plan-for-the-data",
    "relUrl": "/projects/fndm2/datanarrative/#plan-for-the-data"
  },"37": {
    "doc": "Data Narrative",
    "title": "Data Acquisition",
    "content": ". | Data acquired by Dan Wolf &amp; Ted Satterthwaite | Describe the data: . | number of subjects = 103 | types of images = bold (4 runs task-ITC, rest), T1w, fieldmaps | . | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#data-acquisition",
    "relUrl": "/projects/fndm2/datanarrative/#data-acquisition"
  },"38": {
    "doc": "Data Narrative",
    "title": "Download and Storage",
    "content": ". | Original data available on Flywheel | Source data (NIfTI) on CUBIC in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/fndm2. | Data was copied to /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm2 and checked in to datalad after removing PHI (below) | JSON’s within origial_data were updated using cubids-add-nifti-info. | Listing metadata fields using cubids-print-metadata-fields gave the following fields: | . Acknowledgements AcquisitionDateTime AcquisitionMatrixPE AcquisitionNumber AcquisitionTime Authors BIDSVersion BandwidthPerPixelPhaseEncode BaseResolution CoilString ConversionSoftware ConversionSoftwareVersion DatasetDOI DeidentificationMethod DerivedVendorReportedEchoSpacing DeviceSerialNumber DwellTime EchoNumber EchoTime EchoTime1 EchoTime2 EchoTrainLength EffectiveEchoSpacing FlipAngle Funding HowToAcknowledge ImageOrientationPatientDICOM ImageType ImagingFrequency InPlanePhaseEncodingDirectionDICOM InstitutionAddress InstitutionName InstitutionalDepartmentName IntendedFor InversionTime License MRAcquisitionType MagneticFieldStrength Manufacturer ManufacturersModelName Modality Name ParallelReductionFactorInPlane PartialFourier PatientPosition PatientSex PercentPhaseFOV PhaseEncodingDirection PhaseEncodingSteps PhaseResolution PixelBandwidth ProcedureStepDescription ProtocolName PulseSequenceDetails ReceiveCoilName ReconMatrixPE RefLinesPE ReferencesAndLinks RepetitionTime SAR ScanOptions ScanningSequence SequenceName SequenceVariant SeriesDescription SeriesInstanceUID SeriesNumber ShimSetting SliceThickness SliceTiming SoftwareVersions SpacingBetweenSlices StationName StudyID StudyInstanceUID TaskName TotalReadoutTime TxRefAmp template ImageComments MultibandAccelerationFactor . Offending fields were removed with cubids-remove-metadata-fields in /cbica/projects/wolf_satterthwaite_reward/Curation/code/metadatafields_remove.sh . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#download-and-storage",
    "relUrl": "/projects/fndm2/datanarrative/#download-and-storage"
  },"39": {
    "doc": "Data Narrative",
    "title": "Curation Process",
    "content": ". | Data curation by Tinashe Tapera on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iteration7/fndm2/ | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#curation-process",
    "relUrl": "/projects/fndm2/datanarrative/#curation-process"
  },"40": {
    "doc": "Data Narrative",
    "title": "BIDS Validation:",
    "content": ". | Data with short bold time series (&lt;3mins) were removed with the Notebook /cbica/projects/wolf_satterthwaite_reward/Curation/code/RemoveShortBOLD.ipynb . | All validation outputs are available in chronological order in /cbica/projects/wolf_satterthwaite_reward/Curation/code/validate_outputs/fndm2; most recent validation errors being: . | EVENTS_TSV_MISSING ( Task scans should have a corresponding events.tsv file. If this is a resting state scan you can ignore this warning or rename the task to include the word “rest”. ) : 386 counts . | README_FILE_MISSING ( The recommended file /README is missing. See Section 03 (Modality agnostic files) of the BIDS specification. ) : 1 count . | NO_AUTHORS ( The Authors field of dataset_description.json should contain an array of fields - with one author per field. This was triggered because there are no authors, which will make DOI registration from dataset metadata impossible. ) : 1 count . | . | . Data at this stage were approved for preprocessing. ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#bids-validation",
    "relUrl": "/projects/fndm2/datanarrative/#bids-validation"
  },"41": {
    "doc": "Data Narrative",
    "title": "BIDS Optimization:",
    "content": ". | All cubids optimization results are available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter&lt;ITERATION_NUMBER&gt;/fndm2/ . | Final optimization resulted in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter6/fndm2/fndm2_summary.csv . | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#bids-optimization",
    "relUrl": "/projects/fndm2/datanarrative/#bids-optimization"
  },"42": {
    "doc": "Data Narrative",
    "title": "Preprocessing Pipelines",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#preprocessing-pipelines",
    "relUrl": "/projects/fndm2/datanarrative/#preprocessing-pipelines"
  },"43": {
    "doc": "Data Narrative",
    "title": "fMRIPrep (version 20.2.3)",
    "content": ". | Tinashe Tapera was responsible for running preprocessing pipelines/audits on CUBIC | . Exemplar Testing: . | Used cubids to create exemplar dataset: cubids-copy-exemplars /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm2/BIDS /cbica/projects/wolf_satterthwaite_reward/Testing/fndm2/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter6/fndm2/fndm2_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Testing/fndm2/exemplars_dir | Path to fmriprep container: Original in ~/dropbox, datalad in /cbica/projects/wolf_satterthwaite_reward/Testing/exemplars_test/fmriprep-container . | Adjustments: . | During testing, some fieldmaps were found to be corrupt/unusable for the data, and were removed. These files are in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/cubids-testing_adjustments/fndm2/purge_no_fmaps.txt | . | . Testing directory was deleted to save space on CUBIC on 12/2/21, once production completed . Production Testing: . | 102/103 subjects completed fMRIPrep successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm2/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/fmriprep/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/fmriprep/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/fmriprep/-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: NA | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#fmriprep-version-2023",
    "relUrl": "/projects/fndm2/datanarrative/#fmriprep-version-2023"
  },"44": {
    "doc": "Data Narrative",
    "title": "XCP-ABCD (version 0.0.8)",
    "content": "Production Testing: . | 102/103 subjects completed XCP successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/fndm2/fmriprep/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/xcp/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/xcp/output_ria | Path to xcp production audit: NA | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Production/fndm2/xcp-derivatives/XCP | Path to xcp derivatives (concatenated): NA | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#xcp-abcd-version-008",
    "relUrl": "/projects/fndm2/datanarrative/#xcp-abcd-version-008"
  },"45": {
    "doc": "Data Narrative",
    "title": "Post Processing",
    "content": ". | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | . To Do . | backup to PMACS | Add task events files | . ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/#post-processing",
    "relUrl": "/projects/fndm2/datanarrative/#post-processing"
  },"46": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/fndm2/datanarrative/",
    "relUrl": "/projects/fndm2/datanarrative/"
  },"47": {
    "doc": "Data Narrative",
    "title": "CogTrain",
    "content": ". | Data Processing Flow &amp; Important Links: | Plan for the Data | Data Acquisition | Download and Storage | Curation Process . | BIDS Validation: | BIDS Optimization: | . | Preprocessing Pipelines . | fMRIPrep (version 20.2.3) . | Exemplar Testing: | Production Testing: | . | XCP-ABCD (version 0.0.8) . | Production Testing: | . | . | Post Processing . | To Do | . | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#cogtrain",
    "relUrl": "/projects/cogtrain/datanarrative/#cogtrain"
  },"48": {
    "doc": "Data Narrative",
    "title": "Data Processing Flow &amp; Important Links:",
    "content": ". | Flow diagram that describes the lifecycle of this dataset curation and preprocessing: | . NA . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#data-processing-flow--important-links",
    "relUrl": "/projects/cogtrain/datanarrative/#data-processing-flow--important-links"
  },"49": {
    "doc": "Data Narrative",
    "title": "Plan for the Data",
    "content": ". | Why does PennLINC need this data? Acquired at UPenn . | For which project(s) is it intended? Please link to project pages below: Reward Project . | Goal: Curate and preprocess an amalgam of datasets for a harmonized PennLINC resource . | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#plan-for-the-data",
    "relUrl": "/projects/cogtrain/datanarrative/#plan-for-the-data"
  },"50": {
    "doc": "Data Narrative",
    "title": "Data Acquisition",
    "content": ". | Data acquired by Joe Kable | Describe the data: . | number of subjects = 166 | types of images = bold (4 runs task-RISK, 4 runs task-ITC, rest), T1w, fieldmaps, DWI | . | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#data-acquisition",
    "relUrl": "/projects/cogtrain/datanarrative/#data-acquisition"
  },"51": {
    "doc": "Data Narrative",
    "title": "Download and Storage",
    "content": ". | Original data available from Kable Lab | Source data (NIfTI) on CUBIC in /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/CogTrain. | Data was copied to /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/CogTrain and checked in to datalad after removing PHI (below) | JSON’s within origial_data were updated using cubids-add-nifti-info. | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#download-and-storage",
    "relUrl": "/projects/cogtrain/datanarrative/#download-and-storage"
  },"52": {
    "doc": "Data Narrative",
    "title": "Curation Process",
    "content": ". | Data curation by Tinashe Tapera on the CUBIC project user wolfsatterthwaitereward | Link to final CuBIDS csvs: NA | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#curation-process",
    "relUrl": "/projects/cogtrain/datanarrative/#curation-process"
  },"53": {
    "doc": "Data Narrative",
    "title": "BIDS Validation:",
    "content": ". | BIDS validation output at /cbica/projects/wolf_satterthwaite_reward/Curation/code/validate_outputs/CogTrain/CogTrain_validation_2.csv: . | INCONSISTENT_PARAMETERS ( Not all subjects/sessions/runs have the same scanning parameters. ) : 244 counts . | PARTICIPANT_ID_PATTERN (Participant_id column labels must consist of the pattern “sub-\".) : 166 . | . | . Data at this stage were approved for preprocessing. ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#bids-validation",
    "relUrl": "/projects/cogtrain/datanarrative/#bids-validation"
  },"54": {
    "doc": "Data Narrative",
    "title": "BIDS Optimization:",
    "content": ". | All cubids optimization results are available in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter&lt;ITERATION_NUMBER&gt;/CogTrain/ . | Final optimization resulted in /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter1/CogTrain/CogTrain_summary.csv . | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#bids-optimization",
    "relUrl": "/projects/cogtrain/datanarrative/#bids-optimization"
  },"55": {
    "doc": "Data Narrative",
    "title": "Preprocessing Pipelines",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#preprocessing-pipelines",
    "relUrl": "/projects/cogtrain/datanarrative/#preprocessing-pipelines"
  },"56": {
    "doc": "Data Narrative",
    "title": "fMRIPrep (version 20.2.3)",
    "content": ". | Tinashe Tapera was responsible for running preprocessing pipelines/audits on CUBIC | . Exemplar Testing: . | Used cubids to create exemplar dataset: cubids-copy-exemplars /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/CogTrain/BIDS /cbica/projects/wolf_satterthwaite_reward/Testing/CogTrain/exemplars_dir /cbica/projects/wolf_satterthwaite_reward/Curation/code/iterations/iter1/CogTrain/CogTrain_AcqGrouping.csv | Path to exemplar dataset (annexed to datalad): /cbica/projects/wolf_satterthwaite_reward/Testing/CogTrain/exemplars_dir | Path to fmriprep container: Original in ~/dropbox, datalad in /cbica/projects/wolf_satterthwaite_reward/Testing/exemplars_test/fmriprep-container | . Testing directory was deleted to save space on CUBIC on 12/2/21, once production completed . Production Testing: . | 291/293 sessions completed fMRIPrep successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/CogTrain/BIDS | Path to fmriprep run command: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/fmriprep-multises/analysis/code/fmriprep_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/fmriprep-multises/output_ria | Path to fmriprep production audit: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/fmriprep-multises-audit/FMRIPREP_AUDIT.csv | Path to freesurfer production audit: NA | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#fmriprep-version-2023",
    "relUrl": "/projects/cogtrain/datanarrative/#fmriprep-version-2023"
  },"57": {
    "doc": "Data Narrative",
    "title": "XCP-ABCD (version 0.0.8)",
    "content": "Production Testing: . | 289/293 sessions completed XCP successfully | Path to production inputs: /cbica/projects/wolf_satterthwaite_reward/Curation/bidsdatasets/CogTrain/fmriprep-multises/merge_ds | Path to xcp run command: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/xcp-multises/analysis/code/xcp_zip.sh | Path to production outputs: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/xcp-multises/output_ria | Path to xcp production audit: NA | Path to xcp derivatives: /cbica/projects/wolf_satterthwaite_reward/Production/CogTrain/xcp-derivatives/XCP | Path to xcp derivatives (concatenated): NA | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#xcp-abcd-version-008",
    "relUrl": "/projects/cogtrain/datanarrative/#xcp-abcd-version-008"
  },"58": {
    "doc": "Data Narrative",
    "title": "Post Processing",
    "content": ". | Who is using the data/for which projects are people in the lab using this data? . | Link to project page(s) here | . | For each post-processing analysis that has been run on this data, fill out the following . | Who performed the analysis? | Where it was performed (CUBIC, PMACS, somewhere else)? | GitHub Link(s) to result(s) | Did you use pennlinckit? . | https://github.com/PennLINC/PennLINC-Kit/tree/main/pennlinckit | . | . | . To Do . | backup to PMACS | Add task events files | . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/#post-processing",
    "relUrl": "/projects/cogtrain/datanarrative/#post-processing"
  },"59": {
    "doc": "Data Narrative",
    "title": "Data Narrative",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/datanarrative/",
    "relUrl": "/projects/cogtrain/datanarrative/"
  },"60": {
    "doc": "Day2",
    "title": "Day2",
    "content": "This section outlines progress with the Day2 project. ",
    "url": "http://localhost:4000/Reward/projects/day2/day2_index/",
    "relUrl": "/projects/day2/day2_index/"
  },"61": {
    "doc": "FNDM1",
    "title": "FNDM1",
    "content": "This section outlines progress with the FNDM1 project. ",
    "url": "http://localhost:4000/Reward/projects/fndm1/fndm1_index/",
    "relUrl": "/projects/fndm1/fndm1_index/"
  },"62": {
    "doc": "FNDM2",
    "title": "FNDM2",
    "content": "This section outlines progress with the FNDM2 project. ",
    "url": "http://localhost:4000/Reward/projects/fndm2/fndm2_index/",
    "relUrl": "/projects/fndm2/fndm2_index/"
  },"63": {
    "doc": "Home",
    "title": "Reward Documentation",
    "content": "This site documents the progress of curation and preprocessing of the Reward datasets on CUBIC. This documentation continues on from Anna Xu’s curation on Flywheel (see here). ",
    "url": "http://localhost:4000/Reward/#reward-documentation",
    "relUrl": "/#reward-documentation"
  },"64": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "http://localhost:4000/Reward/",
    "relUrl": "/"
  },"65": {
    "doc": "E-Prime",
    "title": "Parsing E-Prime Files",
    "content": "This sub-project documents the processing of the e-prime behavioural files. A fully documented tutorial of how this is accomplished, with accompanying commentary, is available at pennlinc.github.io (this is just the initial attempt that prompted the tutorial documentation). E-prime files were shared with collaborators at the Kable lab, and are still available at /cbica/projects/wolf_satterthwaite_reward/Curation/code/itc_events_BIDS_tree. ",
    "url": "http://localhost:4000/Reward/projects/itc_eprime/itc_eprime/#parsing-e-prime-files",
    "relUrl": "/projects/itc_eprime/itc_eprime/#parsing-e-prime-files"
  },"66": {
    "doc": "E-Prime",
    "title": "E-Prime",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/itc_eprime/itc_eprime/",
    "relUrl": "/projects/itc_eprime/itc_eprime/"
  },"67": {
    "doc": "ITC Task Events",
    "title": "Processing Task Event Files",
    "content": "In addition to preprocessing, we also attempted to parse ITC events files into BIDS valid format. The notebook with progress is available here . ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/itc_task_events/#processing-task-event-files",
    "relUrl": "/projects/cogtrain/itc_task_events/#processing-task-event-files"
  },"68": {
    "doc": "ITC Task Events",
    "title": "ITC Task Events",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/cogtrain/itc_task_events/",
    "relUrl": "/projects/cogtrain/itc_task_events/"
  },"69": {
    "doc": "NEFF V1 & V2",
    "title": "NEFF",
    "content": "NEFF sub-project did not end up getting off the ground in time. Preliminary BIDs data was downloaded at /cbica/projects/wolf_satterthwaite_reward/original_data/bidsdatasets/neff*. The NEFF project spanned a small handful of separate and overlapping imaging protocols, and so was left to the end to parse. Ultimately, the task was not completed. ",
    "url": "http://localhost:4000/Reward/projects/neff/neff_index/#neff",
    "relUrl": "/projects/neff/neff_index/#neff"
  },"70": {
    "doc": "NEFF V1 & V2",
    "title": "NEFF V1 & V2",
    "content": " ",
    "url": "http://localhost:4000/Reward/projects/neff/neff_index/",
    "relUrl": "/projects/neff/neff_index/"
  },"71": {
    "doc": "Nodra",
    "title": "Nodra",
    "content": "This section outlines progress with the Nodra project. ",
    "url": "http://localhost:4000/Reward/projects/nodra/nodra_index/",
    "relUrl": "/projects/nodra/nodra_index/"
  },"72": {
    "doc": "Parsing EPrime Files",
    "title": "Processing",
    "content": "We want to get this into BIDS standard. Here’s the proposed columns: . Mandatory in BIDS: . onset: time that the trial happened from the start of the scan . duration: how long that event happened for . Required for ITC . choice: did they choose the delayed offer or not . button_press: did they go left or right . amount: how much they were offered . delay: what was the delay on the offer . reaction time: how long did they take to make their decision . ",
    "url": "http://localhost:4000/Reward/projects/itc_eprime/parse_eprime/#processing",
    "relUrl": "/projects/itc_eprime/parse_eprime/#processing"
  },"73": {
    "doc": "Parsing EPrime Files",
    "title": "Data Wrangling",
    "content": "The first thing we need to define is what choice they made. The column LeftRight indicates whether the instant $20 option is on the left or right of the screen. If LeftRight == 1, the delayed option was presented on the left. We’ll code this as a factor delay_position with two levels, 1 to indicate it was on the right, and 0 to indicate it was on the left: . trial_df_proc &lt;- trial_df %&gt;% select(-contains(\"null\"), -contains(\"Eprime\"), -contains(\"Feedback\")) %&gt;% mutate(delay_position = ifelse(LeftRight == 0, 1, 0)) . Next, we define which button they pressed, left (0) or right (1): . We’ll also define the motor response they made, button_press, as either 1 (right) or 0 (left). We use CHOICE.RESP and CHOICE1.RESP as the indicator of which side they picked. If the delayed option was on the left, use CHOICE.RESP; r is right, y is left. Then, we encode the choice as either 1=delayed (the delayed position and the chosen button press where the same) or 0=now (the delayed position and the chosen button press where different): . trial_df_proc &lt;- trial_df_proc %&gt;% mutate( button_press = case_when( LeftRight == 1 &amp; Choice.RESP == \"y\" ~ 0, LeftRight == 1 &amp; Choice.RESP == \"r\" ~ 1, LeftRight == 0 &amp; Choice1.RESP == \"y\" ~ 0, LeftRight == 0 &amp; Choice1.RESP == \"r\" ~ 1 ), choice = case_when( LeftRight == 1 &amp; Choice.RESP == \"y\" ~ 1, LeftRight == 1 &amp; Choice.RESP == \"r\" ~ 0, LeftRight == 0 &amp; Choice1.RESP == \"y\" ~ 0, LeftRight == 0 &amp; Choice1.RESP == \"r\" ~ 1 ) ) . We also include the amount and delay: . trial_df_proc &lt;- trial_df_proc %&gt;% rename(offer = Offer) %&gt;% select(offer, delay, choice, button_press, delay_position, everything()) . The duration of the trial is uniform at 4000ms. There are two columns for event timings: Choice1.OnsetTime and Choice.OnsetTime. If LeftRight==1 the task proceeds to Choice.*. If they clicked right, the time for this gets recorded in Choice.OnsetTime, and the value in Choice1.OnsetTime is duplicated from the previous row: . 10 216687 222699 ## left 11 228727 222699 ## left 12 246744 222699 ## right 13 246744 264762 ## right 14 246744 270774 ## right . Here we use the duplicated function to tell us if a value in a vector is a duplicate of itself. If there’s a duplicate, take the value from the opposite column: . trial_df_proc &lt;- trial_df_proc %&gt;% mutate( onset = case_when( # the first row should both not be duplicates !duplicated(Choice.OnsetTime) &amp; !duplicated(Choice1.OnsetTime) ~ 0, # if any value in Choice is a duplicate, choose the value from Choice1 duplicated(Choice.OnsetTime) ~ Choice1.OnsetTime, # vice versa duplicated(Choice1.OnsetTime) ~ Choice.OnsetTime ), duration = 4000, ) %&gt;% select(onset, duration, everything()) . We can do the same for response time: . trial_df_proc &lt;- trial_df_proc %&gt;% mutate( row_num = row_number(), response_time = case_when( # the first row should be the one that's not zero row_num == 1 &amp; Choice.RT == 0 ~ Choice1.RT, row_num == 1 &amp; Choice1.RT == 0 ~ Choice.RT, # if any value in Choice is a duplicate, choose the value from Choice1 duplicated(Choice.RT) ~ Choice1.RT, # vice versa duplicated(Choice1.RT) ~ Choice.RT ) ) %&gt;% select(onset, duration, response_time, everything(), -row_num) . Lastly, we have to account for the timepoint that the task began (as opposed to when the participant is reading instructions). This is given in the slides_df: . trial_df_proc &lt;- trial_df_proc %&gt;% mutate(onset = ifelse(onset == 0, onset, onset - slides_df$Slide1.OffsetTime[2])) . Here’s the data head so far: . trial_df_sv &lt;- trial_df_proc %&gt;% select(onset:delay_position) %&gt;% mutate(IA = 20) trial_df_sv %&gt;% head() ## onset duration response_time offer delay choice button_press delay_position ## 1 0 4000 2328 23.0 21 0 0 1 ## 2 14000 4000 1672 28.0 5 1 0 0 ## 3 23000 4000 2016 30.5 55 0 1 0 ## 4 44000 4000 1560 39.0 34 0 1 0 ## 5 56000 4000 2264 25.0 7 0 0 1 ## 6 62000 4000 1416 22.5 45 0 1 0 ## IA ## 1 20 ## 2 20 ## 3 20 ## 4 20 ## 5 20 ## 6 20 . From here, we write this data frame to a file. trial_df_sv %&gt;% write_delim(\"/cbica/projects/wolf_satterthwaite_reward/Curation/code/itc_eprime/matlab_code/data/demo_trial.txt\", delim = \"\\t\") . ",
    "url": "http://localhost:4000/Reward/projects/itc_eprime/parse_eprime/#data-wrangling",
    "relUrl": "/projects/itc_eprime/parse_eprime/#data-wrangling"
  },"74": {
    "doc": "Parsing EPrime Files",
    "title": "Subjective Value",
    "content": "We calculate the k parameter for ITC using a pre-compiled matlab script: . function []=kable_itc_wrapper(inputfile, outputfile) m = readtable(inputfile); type='h'; choice = m.('choice'); IA = m.('IA'); DA = m.('offer'); D = m.('delay'); out = KableLab_ITC(type,choice,IA,DA,D); k_param=out.k; Tout = table(k_param); writetable(Tout,outputfile); disp(k_param); end % ------------------------------------- % % kable lab ITC code starts here % ------------------------------------- % function out = KableLab_ITC(type,choice,IA,DA,D) % data quality control by checking missed trials. if sum(choice ~= 0 &amp; choice ~= 1) ~= 0 error('choice input has non binary elements') end if sum(choice) == length(choice) || sum(choice) == 0 error('choices are all one-sided') end % type specifies the type of discounting function if strcmp(type,'e') || strcmp(type,'h') out = fitoneparam(type,choice,IA,DA,D); %elseif strcmp(type,'g') % out = fitgen(choice,IA,DA,D); %elseif strcmp(type,'q') % out = fitquasi(choice,IA,DA,D); else error('unknown discounting function type') end out.type = type; [~,dev] = mnrfit([],2-choice); % an intercept-only model out.LL0 = -dev/2; % LL of a null model out.r2 = 1 - out.LL/out.LL0; % McFadden's R-squared out.DV = out.SVlater-IA; % Decision Variable predictedChoice = out.DV&gt;=0; out.percentPredicted = sum(predictedChoice == choice) / length(choice) * 100; out.choicep = 1 ./ (1 + exp(-(out.noise.*out.DV))); out.choiceresid = out.choicep-choice; out.rawresid = out.DV-round(max(abs(out.DV))).*(2.*choice-1); end function out = fitoneparam(type,choice,IA,DA,D) if strcmp(type,'e') % exponential case indiffk = -log(IA./DA)./D; else % hyperbolic case indiffk = (DA-IA)./(IA.*D); end mink = min(indiffk)*(0.99); maxk = max(indiffk)*(1.01); [noise,ks] = meshgrid([-1,0,1], linspace(log(mink),log(maxk),5)); % search grid. We search in logspace b = [noise(:) ks(:)]; info.negLL = inf; for i = 1:length(b) [new.b,new.negLL] = fmincon(@negLL,b(i,:),[],[],[],[],[log(eps),log(mink)],[2,log(maxk)],[],... optimset('Algorithm','interior-point','Display','off'),choice,IA,DA,D,type); if new.negLL &lt; info.negLL info = new; end end out.k = exp(info.b(2)); out.noise = exp(info.b(1)); out.LL = -info.negLL; if strcmp(type,'e') out.SVlater = DA.*exp(-out.k.*D); else out.SVlater = DA./(1+out.k.*D); end end function negLL = negLL(beta,choice,IA,DA,D,type) % function for negative Log-Likelihood switch type case 'e' SVdelayed = DA.*exp(-exp(beta(2)).*D); case 'h' SVdelayed = DA./(1+exp(beta(2)).*D); case 'q' SVdelayed = exp(log(DA)+log(beta(2))+D.*log(beta(3))); beta(1) = log(beta(1)); case 'g' SVdelayed = DA./((1+exp(beta(2)).*D).^beta(3)); beta(1) = log(beta(1)); end reg = exp(beta(1)).*(SVdelayed-IA); p = 1 ./ (1 + exp(-reg)); p(p == 1) = 1-eps; p(p == 0) = eps; negLL = -sum((choice==1).*log(p) + (choice==0).*log(1-p)); end . Here’s how it’s called: ./run_kable_itc_wrapper.sh $MATLAB_DIR data/demo_trial.txt data/output_trial.txt . And read it in: . k_param &lt;- read_table(\"/cbica/projects/wolf_satterthwaite_reward/Curation/code/itc_eprime/matlab_code/data/output_trial.txt\") %&gt;% pull(k_param) ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## k_param = col_double() ## ) k_param ## [1] 0.07106205 . Finally, subjective value is calculated by \\(\\frac{\\text{offer}} {1 + k \\times \\text{delay}}\\) . final_df &lt;- trial_df_sv %&gt;% mutate(subjective_value = offer / (1 + k_param * delay)) %&gt;% mutate_if(~ is.numeric(.) &amp;&amp; all(unique(.) %in% c(0, 1, NA)), factor) %&gt;% select(-IA) . ",
    "url": "http://localhost:4000/Reward/projects/itc_eprime/parse_eprime/#subjective-value",
    "relUrl": "/projects/itc_eprime/parse_eprime/#subjective-value"
  },"75": {
    "doc": "Parsing EPrime Files",
    "title": "Final Outputs",
    "content": "Here’s the final dataframe: . final_df ## onset duration response_time offer delay choice button_press delay_position ## 1 0 4000 2328 23.0 21 0 0 1 ## 2 14000 4000 1672 28.0 5 1 0 0 ## 3 23000 4000 2016 30.5 55 0 1 0 ## 4 44000 4000 1560 39.0 34 0 1 0 ## 5 56000 4000 2264 25.0 7 0 0 1 ## 6 62000 4000 1416 22.5 45 0 1 0 ## 7 68000 4000 1344 37.5 149 0 1 0 ## 8 83000 4000 1200 40.5 97 0 1 0 ## 9 95000 4000 1656 31.5 68 0 0 1 ## 10 101000 4000 1168 46.5 48 0 1 0 ## 11 107000 4000 1544 31.5 48 0 0 1 ## 12 125000 4000 1536 27.5 53 0 0 1 ## 13 143000 4000 1224 43.0 161 0 1 0 ## 14 149000 4000 1536 48.0 107 0 1 0 ## 15 161000 4000 3464 41.5 15 1 1 1 ## 16 170000 4000 1248 39.0 48 0 0 1 ## 17 185000 4000 1048 25.5 149 0 1 0 ## 18 191000 4000 1344 20.5 84 0 0 1 ## 19 197000 4000 1352 38.0 178 0 0 1 ## 20 203000 4000 3168 31.5 7 0 0 1 ## 21 209000 4000 1072 25.5 84 0 1 0 ## 22 215000 4000 1312 34.0 7 1 1 1 ## 23 221000 4000 1272 20.5 161 0 0 1 ## 24 233000 4000 1696 41.5 7 1 0 0 ## 25 254000 4000 928 28.0 48 0 1 0 ## 26 263000 4000 1216 29.0 4 1 1 1 ## 27 269000 4000 1128 33.0 55 0 1 0 ## 28 278000 4000 1048 47.5 43 0 0 1 ## 29 290000 4000 1112 33.0 107 0 0 1 ## 30 296000 4000 824 22.5 178 0 1 0 ## 31 302000 4000 1016 50.0 124 0 1 0 ## 32 308000 4000 1192 30.0 34 0 0 1 ## 33 317000 4000 1424 26.5 7 0 1 0 ## 34 341000 4000 1032 25.5 124 0 1 0 ## 35 347000 4000 880 39.0 124 0 1 0 ## 36 356000 4000 1136 25.5 107 0 0 1 ## 37 365000 4000 1456 36.5 32 0 1 0 ## 38 371000 4000 1128 41.5 178 0 0 1 ## 39 380000 4000 760 45.0 149 0 1 0 ## 40 386000 4000 1096 44.0 124 0 0 1 ## 41 395000 4000 1336 29.0 68 0 0 1 ## 42 410000 4000 1488 29.0 17 0 0 1 ## 43 419000 4000 2328 26.5 29 0 0 1 ## 44 431000 4000 1256 48.0 23 0 0 1 ## 45 449000 4000 1368 30.5 97 0 0 1 ## 46 455000 4000 1272 44.0 34 0 1 0 ## 47 470000 4000 912 47.5 84 0 1 0 ## 48 482000 4000 1328 48.0 34 0 1 0 ## 49 491000 4000 1136 22.5 124 0 0 1 ## 50 500000 4000 1136 28.0 124 0 1 0 ## subjective_value ## 1 9.228413 ## 2 20.659477 ## 3 6.213822 ## 4 11.416496 ## 5 16.695223 ## 6 5.359961 ## 7 3.236038 ## 8 5.131117 ## 9 5.401032 ## 10 10.541879 ## 11 7.141273 ## 12 5.769689 ## 13 3.456317 ## 14 5.579035 ## 15 20.087799 ## 16 8.841576 ## 17 2.200506 ## 18 2.941509 ## 19 2.784078 ## 20 21.035981 ## 21 3.658950 ## 22 22.705503 ## 23 1.647779 ## 24 27.714070 ## 25 6.347798 ## 26 22.581305 ## 27 6.723151 ## 28 11.712004 ## 29 3.835586 ## 30 1.648467 ## 31 5.095960 ## 32 8.781920 ## 33 17.696936 ## 34 2.598940 ## 35 3.974849 ## 36 2.963862 ## 37 11.148492 ## 38 3.040506 ## 39 3.883246 ## 40 4.484445 ## 41 4.972378 ## 42 13.133732 ## 43 8.657869 ## 44 18.220280 ## 45 3.864174 ## 46 12.880149 ## 47 6.815692 ## 48 14.051072 ## 49 2.293182 ## 50 2.853738 . We pair this with a dictionary in BIDS describing how to interpret these columns, like: . { \"column\": { \"LongName\": \"some name\", \"Description\": \"long description\", \"Levels\": { \"go\": \"level one of a factor\", \"stop\": \"leve two of a factor\", } } } dictionary &lt;- final_df %&gt;% select(-onset, -duration) %&gt;% map(~ list( LongName = \"\", Description = \"\" )) dictionary$response_time$LongName &lt;- \"Response Time to Offer\" dictionary$response_time$Description &lt;- \"How long it takes for the participant to choose and respond with a button press\" dictionary$offer$LongName &lt;- \"Offer Amount\" dictionary$offer$Description &lt;- \"Amount of money in dollars that the trial is offering\" dictionary$delay$LongName &lt;- \"Delay on Offer\" dictionary$delay$Description &lt;- \"Number of days to wait until they can receive the offer\" dictionary$choice$LongName &lt;- \"Choice made by the participant\" dictionary$choice$Description &lt;- \"Factor indicating whether the participant selected an offer of 20 dollars immediately, or an offer of more value at the delayed time\" dictionary$choice$Levels &lt;- list(`1` = \"Selected the delayed offer\", `0` = \"Selected the immediate offer\") dictionary$button_press$LongName &lt;- \"Which button was pressed\" dictionary$button_press$Description &lt;- \"Factor indicating whether the participant pressed the button on the right or the left\" dictionary$button_press$Levels &lt;- list(`1` = \"Pressed the right hand button\", `0` = \"Pressed the left hand button\") dictionary$delay_position$LongName &lt;- \"Position of the delayed option\" dictionary$delay_position$Description &lt;- \"Factor indicating which side of the screen the delayed option is on for the trial\" dictionary$delay_position$Levels &lt;- list(`1` = \"Delay on the right\", `0` = \"Delay on the left\") dictionary$subjective_value$LongName &lt;- \"Subjective value of potential offer\" dictionary$subjective_value$Description &lt;- \"Participant's subjective value of the delayed offer calculated using a discount curve (as is common in the literature)\" # should add paper link in here jsonlite::toJSON(dictionary) %&gt;% jsonlite::prettify() ## { ## \"response_time\": { ## \"LongName\": [ ## \"Response Time to Offer\" ## ], ## \"Description\": [ ## \"How long it takes for the participant to choose and respond with a button press\" ## ] ## }, ## \"offer\": { ## \"LongName\": [ ## \"Offer Amount\" ## ], ## \"Description\": [ ## \"Amount of money in dollars that the trial is offering\" ## ] ## }, ## \"delay\": { ## \"LongName\": [ ## \"Delay on Offer\" ## ], ## \"Description\": [ ## \"Number of days to wait until they can receive the offer\" ## ] ## }, ## \"choice\": { ## \"LongName\": [ ## \"Choice made by the participant\" ## ], ## \"Description\": [ ## \"Factor indicating whether the participant selected an offer of 20 dollars immediately, or an offer of more value at the delayed time\" ## ], ## \"Levels\": { ## \"1\": [ ## \"Selected the delayed offer\" ## ], ## \"0\": [ ## \"Selected the immediate offer\" ## ] ## } ## }, ## \"button_press\": { ## \"LongName\": [ ## \"Which button was pressed\" ## ], ## \"Description\": [ ## \"Factor indicating whether the participant pressed the button on the right or the left\" ## ], ## \"Levels\": { ## \"1\": [ ## \"Pressed the right hand button\" ## ], ## \"0\": [ ## \"Pressed the left hand button\" ## ] ## } ## }, ## \"delay_position\": { ## \"LongName\": [ ## \"Position of the delayed option\" ## ], ## \"Description\": [ ## \"Factor indicating which side of the screen the delayed option is on for the trial\" ## ], ## \"Levels\": { ## \"1\": [ ## \"Delay on the right\" ## ], ## \"0\": [ ## \"Delay on the left\" ## ] ## } ## }, ## \"subjective_value\": { ## \"LongName\": [ ## \"Subjective value of potential offer\" ## ], ## \"Description\": [ ## \"Participant's subjective value of the delayed offer calculated using a discount curve (as is common in the literature)\" ## ] ## } ## } ## . ",
    "url": "http://localhost:4000/Reward/projects/itc_eprime/parse_eprime/#final-outputs",
    "relUrl": "/projects/itc_eprime/parse_eprime/#final-outputs"
  },"76": {
    "doc": "Parsing EPrime Files",
    "title": "Parsing EPrime Files",
    "content": "This notebook parses the eprime files into BIDS valid events TSVs. if (!require(\"pacman\")) install.packages(\"pacman\") ## Loading required package: pacman pacman::p_load(rprime, dplyr, tidyverse, rjson) . Let’s start with an example: . rp &lt;- read_eprime(\"~/dropbox/rawbx/RTG1_1ITCscanner1LLA-05200-1.txt\") experiment_data &lt;- FrameList(rp) message(\"Structure of eprime:\") ## Structure of eprime: print(preview_frames(experiment_data)) ## ## Eprime.Level Running Procedure ## 1 Header Header ## List of 17 ## $ Eprime.Level : num 1 ## $ Eprime.LevelName : chr \"Header_\" ## $ Eprime.Basename : chr \"RTG1_1ITCscanner1LLA-05200-1\" ## $ Eprime.FrameNumber : chr \"1\" ## $ Procedure : chr \"Header\" ## $ Running : chr \"Header\" ## $ VersionPersist : chr \"1\" ## $ LevelName : chr \"LogLevel10\" ## $ Experiment : chr \"RTG1_1ITCscanner1LLA\" ## $ SessionDate : chr \"06-10-2011\" ## $ SessionTime : chr \"13:30:30\" ## $ SessionTimeUtc : chr \"5:30:30 PM\" ## $ Subject : chr \"05200\" ## $ Session : chr \"1\" ## $ RandomSeed : chr \"1055496015\" ## $ Group : chr \"1\" ## $ Display.RefreshRate: chr \"60.052\" ## - attr(*, \"class\")= chr [1:2] \"EprimeFrame\" \"list\" ## ## Eprime.Level Running Procedure ## 3 TrialList TrialProc ## List of 66 ## $ Eprime.Level : num 3 ## $ Eprime.LevelName : chr \"TrialList_1\" ## $ Eprime.Basename : chr \"RTG1_1ITCscanner1LLA-05200-1\" ## $ Eprime.FrameNumber : chr \"2\" ## $ Procedure : chr \"TrialProc\" ## $ Running : chr \"TrialList\" ## $ Offer : chr \"23\" ## $ delay : chr \"21\" ## $ NullDuration : chr \"5000\" ## $ LeftRight : chr \"0\" ## $ FeedbackDur : chr \"1672\" ## $ Cycle : chr \"1\" ## $ Sample : chr \"1\" ## $ nullscreen.OnsetDelay : chr \"303\" ## $ nullscreen.OnsetTime : chr \"54348\" ## $ nullscreen.DurationError : chr \"-303\" ## $ nullscreen.StartTime : chr \"54047\" ## $ nullscreen.OffsetTime : chr \"58845\" ## $ nullscreen.FinishTime : chr \"58845\" ## $ nullscreen.ActionDelay : chr \"0\" ## $ nullscreen.ActionTime : chr \"54348\" ## $ nullscreen.OffsetDelay : chr \"0\" ## $ Choice1.OnsetDelay : chr \"0\" ## $ Choice1.OnsetTime : chr \"59045\" ## $ Choice1.DurationError : chr \"-999999\" ## $ Choice1.Duration : chr \"4000\" ## $ Choice1.StartTime : chr \"58846\" ## $ Choice1.OffsetTime : chr \"61373\" ## $ Choice1.FinishTime : chr \"61373\" ## $ Choice1.OffsetDelay : chr \"-999999\" ## $ Choice1.RTTime : chr \"61373\" ## $ Choice1.RT : chr \"2328\" ## $ Choice1.RESP : chr \"y\" ## $ Choice1.CRESP : chr \"r\" ## $ FeedbackDisplay6.OnsetDelay : chr \"939\" ## $ FeedbackDisplay6.OnsetTime : chr \"62312\" ## $ FeedbackDisplay6.DurationError : chr \"-939\" ## $ FeedbackDisplay6.Duration : chr \"1672\" ## $ FeedbackDisplay6.StartTime : chr \"62310\" ## $ FeedbackDisplay6.OffsetTime : chr \"62845\" ## $ FeedbackDisplay6.FinishTime : chr \"62845\" ## $ FeedbackDisplay6.OffsetDelay : chr \"0\" ## $ Choice.OnsetDelay : chr \"0\" ## $ Choice.OnsetTime : chr \"0\" ## $ Choice.DurationError : chr \"0\" ## $ Choice.Duration : chr \"4000\" ## $ Choice.StartTime : chr \"0\" ## $ Choice.OffsetTime : chr \"0\" ## $ Choice.FinishTime : chr \"0\" ## $ Choice.TargetOffsetTime : chr \"0\" ## $ Choice.TargetOnsetTime : chr \"0\" ## $ Choice.OffsetDelay : chr \"0\" ## $ Choice.RTTime : chr \"0\" ## $ Choice.RT : chr \"0\" ## $ Choice.RESP : chr \"\" ## $ Choice.CRESP : chr \"\" ## $ FeedbackDisplay3.OnsetDelay : chr \"0\" ## $ FeedbackDisplay3.OnsetTime : chr \"0\" ## $ FeedbackDisplay3.DurationError : chr \"0\" ## $ FeedbackDisplay3.Duration : chr \"0\" ## $ FeedbackDisplay3.StartTime : chr \"0\" ## $ FeedbackDisplay3.OffsetTime : chr \"0\" ## $ FeedbackDisplay3.FinishTime : chr \"0\" ## $ FeedbackDisplay3.TargetOffsetTime: chr \"0\" ## $ FeedbackDisplay3.TargetOnsetTime : chr \"0\" ## $ FeedbackDisplay3.OffsetDelay : chr \"0\" ## - attr(*, \"class\")= chr [1:2] \"EprimeFrame\" \"list\" ## ## Eprime.Level Running Procedure ## 2 BlockList BlockProc ## List of 9 ## $ Eprime.Level : num 2 ## $ Eprime.LevelName : chr \"BlockList_1\" ## $ Eprime.Basename : chr \"RTG1_1ITCscanner1LLA-05200-1\" ## $ Eprime.FrameNumber: chr \"52\" ## $ Procedure : chr \"BlockProc\" ## $ Running : chr \"BlockList\" ## $ PracticeMode : chr \"?\" ## $ Cycle : chr \"1\" ## $ Sample : chr \"1\" ## - attr(*, \"class\")= chr [1:2] \"EprimeFrame\" \"list\" ## ## Eprime.Level Running Procedure ## 1 &lt;NA&gt; &lt;NA&gt; ## List of 65 ## $ Eprime.Level : num 1 ## $ Eprime.LevelName : logi NA ## $ Eprime.Basename : chr \"RTG1_1ITCscanner1LLA-05200-1\" ## $ Eprime.FrameNumber : chr \"53\" ## $ Procedure : logi NA ## $ Running : logi NA ## $ Experiment : chr \"RTG1_1ITCscanner1LLA\" ## $ SessionDate : chr \"06-10-2011\" ## $ SessionTime : chr \"13:30:30\" ## $ SessionTimeUtc : chr \"5:30:30 PM\" ## $ Subject : chr \"05200\" ## $ Session : chr \"1\" ## $ RandomSeed : chr \"1055496015\" ## $ Group : chr \"1\" ## $ Display.RefreshRate : chr \"60.052\" ## $ Slide2.OnsetDelay : chr \"19\" ## $ Slide2.OnsetTime : chr \"8774\" ## $ Slide2.DurationError : chr \"-999999\" ## $ Slide2.PreRelease : chr \"0\" ## $ Slide2.Duration : chr \"-1\" ## $ Slide2.StartTime : chr \"8755\" ## $ Slide2.OffsetTime : chr \"19995\" ## $ Slide2.FinishTime : chr \"19995\" ## $ Slide2.TimingMode : chr \"0\" ## $ Slide2.CustomOnsetTime : chr \"0\" ## $ Slide2.CustomOffsetTime : chr \"0\" ## $ Slide2.ActionDelay : chr \"0\" ## $ Slide2.ActionTime : chr \"8774\" ## $ Slide2.TargetOffsetTime : chr \"-1\" ## $ Slide2.TargetOnsetTime : chr \"8755\" ## $ Slide2.OffsetDelay : chr \"-999999\" ## $ Slide2.RTTime : chr \"19995\" ## $ Slide2.ACC : chr \"0\" ## $ Slide2.RT : chr \"11221\" ## $ Slide2.RESP : chr \"n\" ## $ Slide2.CRESP : chr \"\" ## $ Slide1.OnsetTime : chr \"20000\" ## $ Slide1.DurationError : chr \"-999999\" ## $ Slide1.Duration : chr \"-1\" ## $ Slide1.StartTime : chr \"19998\" ## $ Slide1.OffsetTime : chr \"54045\" ## $ Slide1.FinishTime : chr \"54045\" ## $ Slide1.RT : chr \"34045\" ## $ Goodbye.OnsetDelay : chr \"2\" ## $ Goodbye.OnsetTime : chr \"558047\" ## $ Goodbye.DurationError : chr \"0\" ## $ Goodbye.PreRelease : chr \"0\" ## $ Goodbye.Duration : chr \"6000\" ## $ Goodbye.StartTime : chr \"557853\" ## $ Goodbye.OffsetTime : chr \"564047\" ## $ Goodbye.FinishTime : chr \"564047\" ## $ Goodbye.TimingMode : chr \"0\" ## $ Goodbye.CustomOnsetTime : chr \"0\" ## $ Goodbye.CustomOffsetTime: chr \"0\" ## $ Goodbye.ActionDelay : chr \"1\" ## $ Goodbye.ActionTime : chr \"558048\" ## $ Goodbye.TargetOffsetTime: chr \"564047\" ## $ Goodbye.TargetOnsetTime : chr \"558045\" ## $ Goodbye.OffsetDelay : chr \"0\" ## $ Goodbye.RTTime : chr \"0\" ## $ Goodbye.ACC : chr \"0\" ## $ Goodbye.RT : chr \"0\" ## $ Goodbye.RESP : chr \"\" ## $ Goodbye.CRESP : chr \"\" ## $ Clock.StartTimeOfDay : chr \"6/10/2011 1:30:30 PM\" ## - attr(*, \"class\")= chr [1:2] \"EprimeFrame\" \"list\" ## NULL trial &lt;- filter_in(experiment_data, \"Running\", \"TrialList\") block &lt;- filter_in(experiment_data, \"Running\", \"BlockList\") slides &lt;- filter_in(experiment_data, \"Eprime.Level\", 1) slides_df &lt;- to_data_frame(slides) %&gt;% readr::type_convert() ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Eprime.LevelName = col_character(), ## Eprime.Basename = col_character(), ## Procedure = col_character(), ## Running = col_character(), ## LevelName = col_character(), ## Experiment = col_character(), ## SessionDate = col_character(), ## SessionTime = col_time(format = \"\"), ## SessionTimeUtc = col_time(format = \"\"), ## Subject = col_character(), ## Slide2.RESP = col_character(), ## Slide2.CRESP = col_logical(), ## Goodbye.RESP = col_logical(), ## Goodbye.CRESP = col_logical(), ## Clock.StartTimeOfDay = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. trial_df &lt;- to_data_frame(trial) %&gt;% readr::type_convert() ## ## ── Column specification ──────────────────────────────────────────────────────── ## cols( ## .default = col_double(), ## Eprime.LevelName = col_character(), ## Eprime.Basename = col_character(), ## Procedure = col_character(), ## Running = col_character(), ## Choice1.RESP = col_character(), ## Choice1.CRESP = col_character(), ## Choice.RESP = col_character(), ## Choice.CRESP = col_character() ## ) ## ℹ Use `spec()` for the full column specifications. head(trial_df) ## Eprime.Level Eprime.LevelName Eprime.Basename Eprime.FrameNumber ## 1 3 TrialList_1 RTG1_1ITCscanner1LLA-05200-1 2 ## 2 3 TrialList_2 RTG1_1ITCscanner1LLA-05200-1 3 ## 3 3 TrialList_3 RTG1_1ITCscanner1LLA-05200-1 4 ## 4 3 TrialList_4 RTG1_1ITCscanner1LLA-05200-1 5 ## 5 3 TrialList_5 RTG1_1ITCscanner1LLA-05200-1 6 ## 6 3 TrialList_6 RTG1_1ITCscanner1LLA-05200-1 7 ## Procedure Running Offer delay NullDuration LeftRight FeedbackDur Cycle ## 1 TrialProc TrialList 23.0 21 5000 0 1672 1 ## 2 TrialProc TrialList 28.0 5 5000 1 2328 1 ## 3 TrialProc TrialList 30.5 55 5000 1 1984 1 ## 4 TrialProc TrialList 39.0 34 17000 1 2440 1 ## 5 TrialProc TrialList 25.0 7 8000 0 1736 1 ## 6 TrialProc TrialList 22.5 45 2000 1 2584 1 ## Sample nullscreen.OnsetDelay nullscreen.OnsetTime nullscreen.DurationError ## 1 1 303 54348 -303 ## 2 2 0 63045 0 ## 3 3 0 72045 0 ## 4 4 0 81045 0 ## 5 5 0 102045 0 ## 6 6 0 114045 0 ## nullscreen.StartTime nullscreen.OffsetTime nullscreen.FinishTime ## 1 54047 58845 58845 ## 2 62853 67845 67845 ## 3 71854 76845 76845 ## 4 80853 97845 97845 ## 5 101854 109845 109845 ## 6 113853 115845 115845 ## nullscreen.ActionDelay nullscreen.ActionTime nullscreen.OffsetDelay ## 1 0 54348 0 ## 2 0 63045 0 ## 3 0 72045 0 ## 4 0 81045 0 ## 5 0 102045 0 ## 6 0 114045 0 ## Choice1.OnsetDelay Choice1.OnsetTime Choice1.DurationError Choice1.Duration ## 1 0 59045 -999999 4000 ## 2 0 59045 -999999 4000 ## 3 0 59045 -999999 4000 ## 4 0 59045 -999999 4000 ## 5 0 110045 -999999 4000 ## 6 0 110045 -999999 4000 ## Choice1.StartTime Choice1.OffsetTime Choice1.FinishTime Choice1.OffsetDelay ## 1 58846 61373 61373 -999999 ## 2 58846 61373 61373 -999999 ## 3 58846 61373 61373 -999999 ## 4 58846 61373 61373 -999999 ## 5 109846 112309 112309 -999999 ## 6 109846 112309 112309 -999999 ## Choice1.RTTime Choice1.RT Choice1.RESP Choice1.CRESP ## 1 61373 2328 y r ## 2 61373 2328 y r ## 3 61373 2328 y r ## 4 61373 2328 y r ## 5 112309 2264 y r ## 6 112309 2264 y r ## FeedbackDisplay6.OnsetDelay FeedbackDisplay6.OnsetTime ## 1 939 62312 ## 2 939 62312 ## 3 939 62312 ## 4 939 62312 ## 5 3 112312 ## 6 3 112312 ## FeedbackDisplay6.DurationError FeedbackDisplay6.Duration ## 1 -939 1672 ## 2 -939 1672 ## 3 -939 1672 ## 4 -939 1672 ## 5 -3 1736 ## 6 -3 1736 ## FeedbackDisplay6.StartTime FeedbackDisplay6.OffsetTime ## 1 62310 62845 ## 2 62310 62845 ## 3 62310 62845 ## 4 62310 62845 ## 5 112311 113845 ## 6 112311 113845 ## FeedbackDisplay6.FinishTime FeedbackDisplay6.OffsetDelay Choice.OnsetDelay ## 1 62845 0 0 ## 2 62845 0 0 ## 3 62845 0 0 ## 4 62845 0 0 ## 5 113845 0 0 ## 6 113845 0 0 ## Choice.OnsetTime Choice.DurationError Choice.Duration Choice.StartTime ## 1 0 0 4000 0 ## 2 68045 -999999 4000 67846 ## 3 77045 -999999 4000 76846 ## 4 98045 -999999 4000 97847 ## 5 98045 -999999 4000 97847 ## 6 116045 -999999 4000 115846 ## Choice.OffsetTime Choice.FinishTime Choice.TargetOffsetTime ## 1 0 0 0 ## 2 69717 69717 71845 ## 3 79061 79061 80845 ## 4 99605 99605 101845 ## 5 99605 99605 101845 ## 6 117461 117461 119845 ## Choice.TargetOnsetTime Choice.OffsetDelay Choice.RTTime Choice.RT Choice.RESP ## 1 0 0 0 0 &lt;NA&gt; ## 2 68045 -999999 69717 1672 y ## 3 77045 -999999 79061 2016 r ## 4 98045 -999999 99605 1560 r ## 5 98045 -999999 99605 1560 r ## 6 116045 -999999 117461 1416 r ## Choice.CRESP FeedbackDisplay3.OnsetDelay FeedbackDisplay3.OnsetTime ## 1 &lt;NA&gt; 0 0 ## 2 r 3 69720 ## 3 r 3 79064 ## 4 r 3 99608 ## 5 r 3 99608 ## 6 r 3 117464 ## FeedbackDisplay3.DurationError FeedbackDisplay3.Duration ## 1 0 0 ## 2 -3 2328 ## 3 -3 1984 ## 4 -3 2440 ## 5 -3 2440 ## 6 -3 2584 ## FeedbackDisplay3.StartTime FeedbackDisplay3.OffsetTime ## 1 0 0 ## 2 69719 71845 ## 3 79063 80845 ## 4 99607 101845 ## 5 99607 101845 ## 6 117463 119845 ## FeedbackDisplay3.FinishTime FeedbackDisplay3.TargetOffsetTime ## 1 0 0 ## 2 71845 71845 ## 3 80845 80845 ## 4 101845 101845 ## 5 101845 101845 ## 6 119845 119845 ## FeedbackDisplay3.TargetOnsetTime FeedbackDisplay3.OffsetDelay ## 1 0 0 ## 2 69717 0 ## 3 79061 0 ## 4 99605 0 ## 5 99605 0 ## 6 117461 0 . Each line of this table indicates an event (trial) happening in the experiment. In an ITC task, participants are shown a value of money and a delay. They’re asked, “would you rather receive $20 now, or wait X number of days to receive $Y later?” Hence, each line is an offer in this paradigm. ",
    "url": "http://localhost:4000/Reward/projects/itc_eprime/parse_eprime/",
    "relUrl": "/projects/itc_eprime/parse_eprime/"
  }
}
